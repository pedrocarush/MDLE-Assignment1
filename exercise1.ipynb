{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise the A-priori algorithm was developed to get the most frequent itemsets (size 2 and 3), and from them extract association rules. This algorithm was implemented using Spark, more specifically the PySpark library with the Dataframe API."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PySpark is the only non-standard library required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "from pyspark import Broadcast\n",
    "from pyspark.sql import SparkSession, DataFrame, Column\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StringType, ArrayType\n",
    "from itertools import combinations, chain\n",
    "from typing import Iterable, Any, List"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark initialization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark is initialized, with as many worker threads as logical cores on the machine.\n",
    "We did not use a fixed value since the machines used for development had a different number of CPU cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/18 15:22:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName('Apriori') \\\n",
    "    .config('spark.master', 'local[*]') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is medical data, where each row identifies a patient and their disease tested at a certain time.\n",
    "In this context, the diseases are the *items* and the patients are the *baskets*.\n",
    "\n",
    "The data's format is CSV, and is loaded including the header.\n",
    "The `START`, `STOP` and `ENCOUNTER` columns are removed as they are not useful for this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .option('header', True) \\\n",
    "    .csv('./data/conditions.csv.gz') \\\n",
    "    .drop('START', 'STOP', 'ENCOUNTER')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframe `df` will have three columns: `PATIENT`, `CODE` and `DESCRIPTION`.\n",
    "\n",
    "From this dataframe, we extract the mappings from `CODE` to its `DESCRIPTION`.\n",
    "Throughout the algorithm, we will use the `CODE` to identify each disease, and then map it to its `DESCRIPTION` when we output the final results.\n",
    "\n",
    "We first check how many unique diseases there are, so that we can determine whether we can keep this mapping in memory or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Number of distinct CODE-DESCRIPTION pairs:', df.select('CODE', 'DESCRIPTION').distinct().count())\n",
    "# print('Number of distinct CODEs:', df.select('CODE').distinct().count())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is discrepancy between the counts due to one `CODE` having 2 different descriptions.\n",
    "For simplification we simply choose one description over the other.\n",
    "\n",
    "Since there are only 159 diseases, we can perfectly keep the mapping in memory.\n",
    "And so, we collect the distinct `CODE`-`DESCRIPTION` pairs into an hash table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "code_description_map = {r.CODE: r.DESCRIPTION\n",
    "    for r in df \\\n",
    "    .select('CODE', 'DESCRIPTION') \\\n",
    "    .distinct() \\\n",
    "    .collect()\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the algorithm, the diseases' `DESCRIPTION`s won't be needed anymore, as we have the mapping.\n",
    "The distinct `PATIENT`-`CODE` pairs are taken (it doesn't make sense to have duplicate items within a basket)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('DESCRIPTION').distinct()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A-priori algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the support threshold parameter to 1000, as recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_threshold = 1000"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intermediate results of each pass are saved in disk in Parquet format (Spark's default format)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first pass, the frequent items are taken.\n",
    "For that, a \"group by\" operation is performed, grouping by `CODE` and counting the number of `PATIENTS` that each `CODE` is present in.\n",
    "Finally, the diseases are filtered according to the `support_threshold`, by comparing with the support stored in `COUNT`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('frequent_diseases_k1'):\n",
    "    frequent_diseases_k1 = df \\\n",
    "        .groupBy('CODE') \\\n",
    "        .count() \\\n",
    "        .withColumnRenamed('count', 'COUNT') \\\n",
    "        .filter(col('COUNT') >= support_threshold)\n",
    "    \n",
    "    frequent_diseases_k1.write.mode('overwrite').parquet(path='frequent_diseases_k1', compression='gzip')\n",
    "\n",
    "frequent_diseases_k1 = spark.read.parquet('frequent_diseases_k1')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The frequent items table is kept in memory for future passes (in a Python `set`, for quicker membership queries)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_diseases_k1_set = {r.CODE for r in frequent_diseases_k1.select('CODE').collect()}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this set of frequent items will be used multiple times by all nodes in the future, then we can broadcast this read-only data beforehand (https://spark.apache.org/docs/2.2.0/rdd-programming-guide.html#broadcast-variables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not isinstance(frequent_diseases_k1_set, Broadcast):\n",
    "    frequent_diseases_k1_set = spark.sparkContext.broadcast(frequent_diseases_k1_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of frequent items: 131\n"
     ]
    }
   ],
   "source": [
    "print('Number of frequent items:', len(frequent_diseases_k1_set.value))   # 131"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second pass requires generating frequent pairs of items.\n",
    "For that, an UDF was developed that simply took an array of items and returned the list of item pairs, an operation performed within Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(returnType=ArrayType(ArrayType(StringType(), False), False))\n",
    "def combine_pairs(elems: Iterable[Any]):\n",
    "    return list(combinations(elems, 2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the `CODE`s are filtered using the `frequent_diseases_k1_set`, so that we only have frequent diseases (*monotonicity of itemsets*: itemsets are only frequent if all their subsets are).\n",
    "Then, for each `PATIENT` we collect its `CODE`s into an array, and then use that array in the UDF previously defined.\n",
    "\n",
    "It's important to note that the array of `CODE`s should be sorted beforehand, so that pair comparison can be done properly. Since Spark doesn't have a \"set\" datatype, the elements should be kept in order so that two pairs (which are arrays) with the same items will be considered equal.\n",
    "The `combinations` function is guaranteed to keep this order when generating pairs.\n",
    "\n",
    "The result is a column `CODE_PAIRS`, containing an array of pairs, being each pair an array with two elements.\n",
    "This column is exploded, producing a row for each pair within the arrays of pairs.\n",
    "\n",
    "Afterwards, the same grouping procedure in the first pass is performed, grouping by the itemsets and obtaining the number of baskets each itemset belongs to, filtering with the `support_threshold`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('frequent_diseases_k2'):\n",
    "    frequent_diseases_k2 = df \\\n",
    "        .filter(col('CODE').isin(frequent_diseases_k1_set.value)) \\\n",
    "        .groupBy('PATIENT') \\\n",
    "        .agg(collect_list('CODE')) \\\n",
    "        .withColumn('collect_list(CODE)', array_sort('collect_list(CODE)')) \\\n",
    "        .withColumn('CODE_PAIRS', combine_pairs('collect_list(CODE)')) \\\n",
    "        .select('PATIENT', 'CODE_PAIRS') \\\n",
    "        .withColumn('CODE_PAIR', explode('CODE_PAIRS')) \\\n",
    "        .drop('CODE_PAIRS') \\\n",
    "        .groupBy('CODE_PAIR') \\\n",
    "        .count() \\\n",
    "        .withColumnRenamed('count', 'COUNT') \\\n",
    "        .filter(col('COUNT') >= support_threshold)\n",
    "    \n",
    "    frequent_diseases_k2.write.mode('overwrite').parquet(path='frequent_diseases_k2', compression='gzip')\n",
    "\n",
    "frequent_diseases_k2 = spark.read.parquet('frequent_diseases_k2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table of frequent pairs is kept in memory for the third pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_diseases_k2_set = {tuple(r.CODE_PAIR) for r in frequent_diseases_k2.select('CODE_PAIR').collect()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of frequent pairs: 2940\n"
     ]
    }
   ],
   "source": [
    "print('Number of frequent pairs:', len(frequent_diseases_k2_set))   # 2940"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third Pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As was done for the second pass, an UDF was developed that returns an array of triples given an array of items.\n",
    "This function includes the verification that all $k-1$ immediate subsets of each returned triple are frequent (that is, all pairs within the triple are frequent)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(returnType=ArrayType(ArrayType(StringType(), False), False))\n",
    "def combine_triples(elems: Iterable[Any]):\n",
    "    return [\n",
    "        combination for combination in list(combinations(elems, 3))\n",
    "        if ((combination[0], combination[1]) in frequent_diseases_k2_set\n",
    "            and (combination[0], combination[2]) in frequent_diseases_k2_set\n",
    "            and (combination[1], combination[2]) in frequent_diseases_k2_set)\n",
    "    ]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same approach for the second pass was used, merely differing in the UDF used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('frequent_diseases_k3'):\n",
    "    frequent_diseases_k3 = df \\\n",
    "        .filter(col('CODE').isin(frequent_diseases_k1_set.value)) \\\n",
    "        .groupBy('PATIENT') \\\n",
    "        .agg(collect_list('CODE')) \\\n",
    "        .withColumn('collect_list(CODE)', array_sort('collect_list(CODE)')) \\\n",
    "        .withColumn('CODE_TRIPLES', combine_triples('collect_list(CODE)')) \\\n",
    "        .select('PATIENT', 'CODE_TRIPLES') \\\n",
    "        .withColumn('CODE_TRIPLE', explode('CODE_TRIPLES')) \\\n",
    "        .drop('CODE_TRIPLES') \\\n",
    "        .groupBy('CODE_TRIPLE') \\\n",
    "        .count() \\\n",
    "        .withColumnRenamed('count', 'COUNT') \\\n",
    "        .filter(col('COUNT') >= support_threshold)\n",
    "\n",
    "    frequent_diseases_k3.write.mode('overwrite').parquet(path='frequent_diseases_k3', compression='gzip')\n",
    "\n",
    "frequent_diseases_k3 = spark.read.parquet('frequent_diseases_k3')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table of frequent triples is generated, merely because the same was done for the previous $k$, but it won't be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_diseases_k3_set = {tuple(r.CODE_TRIPLE) for r in frequent_diseases_k3.select('CODE_TRIPLE').collect()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of frequent triples: 13395\n"
     ]
    }
   ],
   "source": [
    "print('Number of frequent triples:', len(frequent_diseases_k3_set))   # 13395"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most frequent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For outputting the most frequent itemsets, an UDF was developed merely so the disease `CODE`s are converted to their descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(returnType=ArrayType(StringType(), False))\n",
    "def map_codes_to_description(codes: List[str]):\n",
    "    return [code_description_map[item] for item in codes]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The listing of the 10 frequent pairs/triples is saved in a tab-separated CSV file, which includes the header.\n",
    "Obtaining the 10 most frequent itemsets involves sorting the respective dataframe in descending order and taking the top 10 results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "with open('most_frequent_k2.csv', 'w') as f:\n",
    "    print('pair\\tcount', file=f)\n",
    "    print(*(\n",
    "            f'{r.CODE_PAIR}\\t{r.COUNT}' for r in\n",
    "            frequent_diseases_k2\n",
    "                .withColumn('CODE_PAIR', map_codes_to_description('CODE_PAIR'))\n",
    "                .sort('COUNT', ascending=False).take(10)\n",
    "        ), sep='\\n', file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('most_frequent_k3.csv', 'w') as f:\n",
    "    print('triple\\tcount', file=f)\n",
    "    print(*(\n",
    "            f'{r.CODE_TRIPLE}\\t{r.COUNT}' for r in\n",
    "            frequent_diseases_k3\n",
    "                .withColumn('CODE_TRIPLE', map_codes_to_description('CODE_TRIPLE'))\n",
    "                .sort('COUNT', ascending=False).take(10)\n",
    "        ), sep='\\n', file=f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Association Rules"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout this section, for a rule $A \\rightarrow B$ we denote $A$ as the **LHS** (left-hand side) and $B$ as the **RHS** (right-hand side)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "standardised_lift_threshold = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The total number of patients (baskets) is required for calculating the rule metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "n_patients = df.select('PATIENT').distinct().count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much like in the creation of pairs and triples, an UDF was created so that, taking an itemset as input, produces all association rules with a single element on the RHS.\n",
    "\n",
    "To do so, a list is generated whose elements are tuples of 2 elements, with the first element being the LHS as an array and the RHS as a single-element array.\n",
    "A tuple is generated for every element present in an itemset (a pair or a triple).\n",
    "\n",
    "The LHS has to be sorted so that two LHS arrays with the same elements are considered equal.\n",
    "The RHS is an array merely for generalization, since the RHS could have multiple elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(returnType=ArrayType(ArrayType(ArrayType(StringType(), False), False), False))\n",
    "def generate_association_rules(itemset: List[str]):\n",
    "    itemset = set(itemset)\n",
    "    return [(sorted(itemset - {item}), [item]) for item in itemset]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate rules from the frequent pairs, the previous UDF is applied to the dataframe of item pairs (`frequent_diseases_k2`).\n",
    "The LHS and RHS of the generated subsets is put in two separate columns, `RULE_LHS` and `RULE_RHS` respectively.\n",
    "The support of pairs is kept in `COUNT_PAIR` to be used when calculating the metrics.\n",
    "\n",
    "Afterwards, the dataframe is joined with the dataframe containing the frequent items and their support (`frequent_diseases_k1`) on the disease `CODE`s, so that we can obtain the support of the rule's LHS and RHS.\n",
    "And so, two inner joins are performed, since there it's guaranteed that the rule LHS/RHS is present in the dataframe of frequent items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules_k2 = frequent_diseases_k2 \\\n",
    "    .withColumn('RULES', generate_association_rules('CODE_PAIR')) \\\n",
    "    .withColumn('RULES', explode('RULES')) \\\n",
    "    .select(col('RULES')[0].alias('RULE_LHS'), col('RULES')[1].alias('RULE_RHS'), col('COUNT').alias('COUNT_RULE')) \\\n",
    "    .join(frequent_diseases_k1, frequent_diseases_k1['CODE'] == col('RULE_LHS')[0], 'inner') \\\n",
    "    .withColumnRenamed('COUNT', 'COUNT_LHS') \\\n",
    "    .drop('CODE') \\\n",
    "    .join(frequent_diseases_k1, frequent_diseases_k1['CODE'] == col('RULE_RHS')[0], 'inner') \\\n",
    "    .withColumnRenamed('COUNT', 'COUNT_RHS') \\\n",
    "    .drop('CODE')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the necessary counts on separate columns, the metrics can be easily calculated by referencing the column containing the desired values.\n",
    "The `CONFIDENCE` column is created first because the result of this calculation is used to calculate the other metrics.\n",
    "The results are finally filtered according to the `STANDARDISED_LIFT`.\n",
    "\n",
    "The calculation of metrics is put in a separate function since it will be reused for the rules extracted from triples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_metrics_columns(rule_counts: DataFrame) -> DataFrame:\n",
    "    return rule_counts \\\n",
    "        .withColumn('CONFIDENCE', col('COUNT_RULE') / col('COUNT_LHS')) \\\n",
    "        .withColumn('INTEREST', col('CONFIDENCE') - col('COUNT_RHS') / n_patients) \\\n",
    "        .withColumn('LIFT', n_patients * col('CONFIDENCE') / col('COUNT_RHS')) \\\n",
    "        .withColumn('STANDARDISED_LIFT', \n",
    "                    (col('LIFT') - array_max(array(\n",
    "                        (col('COUNT_LHS') + col('COUNT_RHS')) / n_patients - 1,\n",
    "                        lit(1 / n_patients)\n",
    "                    )) / (col('COUNT_LHS') * col('COUNT_RHS') / (n_patients ** 2)))\n",
    "                    /\n",
    "                    ((n_patients / array_max(array(col('COUNT_LHS'), col('COUNT_RHS')))) - array_max(array(\n",
    "                        (col('COUNT_LHS') + col('COUNT_RHS')) / n_patients - 1,\n",
    "                        lit(1 / n_patients)\n",
    "                    )) / (col('COUNT_LHS') * col('COUNT_RHS') / (n_patients ** 2)))\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules_k2_metrics = rules_k2 \\\n",
    "    .transform(add_metrics_columns) \\\n",
    "    .filter(col('STANDARDISED_LIFT') >= standardised_lift_threshold)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the rules extracted from the frequent triples, a similar approach to the frequent pairs was used, on the dataframe of frequent triples (`frequent_diseases_k3`).\n",
    "\n",
    "However, the join to obtain the LHS support had to be performed differently.\n",
    "Since the rules' LHS can be a subset of different sizes (in this case, it can have one or two elements), the joins have to be performed both with the dataframe of frequent items (`frequent_diseases_k1`) and the dataframe of frequent pairs (`frequent_diseases_k2`).\n",
    "\n",
    "Because of this, the join can't be *inner*, and instead should be a *left join*, so that non-matching `CODE`s aren't removed, but kept with a `null` value on `COUNT`.\n",
    "To obtain the support of the rules' LHS for instance, we join with `frequent_diseases_k1`, and so the rule LHSs that have one element will have a count while the rule LHSs that have two elements will have a `null` value (`COUNT_LHS`).\n",
    "Afterwards, joining with `frequent_diseases_k2` provides the counts for rule LHSs that have two elements, while LHSs with one element have a `null` value (`COUNT_LHS_OTHER`).\n",
    "\n",
    "Both columns `COUNT_LHS` and `COUNT_LHS_OTHER` are \"complementary\" to each other, that is, for a given row a column has a non-`null` value while the other has `null`.\n",
    "In order to combine both into a single column `COUNT_LHS`, `coalesce` is used which uses the first non-`null` value of both columns for all rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules_k3 = frequent_diseases_k3 \\\n",
    "    .withColumn('RULES', generate_association_rules('CODE_TRIPLE')) \\\n",
    "    .withColumn('RULES', explode('RULES')) \\\n",
    "    .select(col('RULES')[0].alias('RULE_LHS'), col('RULES')[1].alias('RULE_RHS'), col('COUNT').alias('COUNT_RULE')) \\\n",
    "    \\\n",
    "    .join(frequent_diseases_k1, array(frequent_diseases_k1['CODE']) == col('RULE_LHS'), 'left') \\\n",
    "    .withColumnRenamed('COUNT', 'COUNT_LHS') \\\n",
    "    .drop('CODE') \\\n",
    "    .join(frequent_diseases_k2, frequent_diseases_k2['CODE_PAIR'] == col('RULE_LHS'), 'left') \\\n",
    "    .withColumnRenamed('COUNT', 'COUNT_LHS_OTHER') \\\n",
    "    .drop('CODE_PAIR') \\\n",
    "    .withColumn('COUNT_LHS', coalesce('COUNT_LHS', 'COUNT_LHS_OTHER')) \\\n",
    "    .drop('COUNT_LHS_OTHER') \\\n",
    "    \\\n",
    "    .join(frequent_diseases_k1, array(frequent_diseases_k1['CODE']) == col('RULE_RHS'), 'inner') \\\n",
    "    .withColumnRenamed('COUNT', 'COUNT_RHS') \\\n",
    "    .drop('CODE')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the counts obtained, calculating the metrics is exactly the same to the rule metrics obtained from the pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules_k3_metrics = rules_k3 \\\n",
    "    .transform(add_metrics_columns) \\\n",
    "    .filter(col('STANDARDISED_LIFT') >= standardised_lift_threshold)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, both dataframes `rules_k2_metrics` and `rules_k3_metrics` are merged into a dataframe containing all their rows.\n",
    "After this, we can sort by the `STANDARDISED_LIFT` for printing the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert rules_k2_metrics.columns == rules_k3_metrics.columns, 'The dataframes of rule metrics should have the same columns in the same order!'\n",
    "\n",
    "rules_metrics = rules_k2_metrics \\\n",
    "    .union(rules_k3_metrics) \\\n",
    "    .sort('STANDARDISED_LIFT', ascending=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate the final results, the disease codes are mapped to their respective descriptions and the columns are transformed into a single `String` column.\n",
    "\n",
    "Then, the dataframe is written to disk, into a partitioned text file.\n",
    "If the file is to be read, then the following command can be used (if on Linux):\n",
    "\n",
    "```bash\n",
    "cat association_rules/part* | less\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(returnType=StringType())\n",
    "def format_rule(rule_1: List[str], rule_2: List[str], *values: List[Any]):\n",
    "    return f'{{{\", \".join(rule_1)}}} -> {{{\", \".join(rule_2)}}}: {\", \".join(map(str, values))}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rules_metrics \\\n",
    "    .withColumn('RULE_LHS', map_codes_to_description('RULE_LHS')) \\\n",
    "    .withColumn('RULE_RHS', map_codes_to_description('RULE_RHS')) \\\n",
    "    .select(format_rule('RULE_LHS', 'RULE_RHS', 'STANDARDISED_LIFT', 'LIFT', 'CONFIDENCE', 'INTEREST').alias('LINE')) \\\n",
    "    .write.mode('overwrite').text('association_rules')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
