{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise the A-priori algorithm was developed to get the most frequent itemsets (size 2 and 3), and from them extract association rules. This algorithm was implemented using Spark, more specifically the PySpark library with the Dataframe API."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PySpark is the only non-standard library required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StringType, ArrayType\n",
    "from itertools import combinations, chain\n",
    "from typing import Iterable, Any, List"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark initialization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark is initialized, with as many worker threads as logical cores on the machine.\n",
    "We did not use a fixed value since the machines used for development had a different number of CPU cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/03/16 21:54:38 WARN Utils: Your hostname, martinho-SATELLITE-L50-B resolves to a loopback address: 127.0.1.1; using 192.168.1.66 instead (on interface enp8s0)\n",
      "23/03/16 21:54:38 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/martinho/.local/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "23/03/16 21:54:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName('Apriori') \\\n",
    "    .config('spark.master', 'local[*]') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is medical data, where each row identifies a patient and their disease tested at a certain time.\n",
    "In this context, the diseases are the *items* and the patients are the *baskets*.\n",
    "\n",
    "The data's format is CSV, and is loaded including the header.\n",
    "The `START`, `STOP` and `ENCOUNTER` columns are removed as they are not useful for this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read \\\n",
    "    .option('header', True) \\\n",
    "    .csv('./data/conditions.csv.gz') \\\n",
    "    .drop('START', 'STOP', 'ENCOUNTER')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframe `df` will have three columns: `PATIENT`, `CODE` and `DESCRIPTION`.\n",
    "\n",
    "From this dataframe, we extract the mappings from `CODE` to its `DESCRIPTION`.\n",
    "Throughout the algorithm, we will use the `CODE` to identify each disease, and then map it to its `DESCRIPTION` when we output the final results.\n",
    "\n",
    "We first check how many unique diseases there are, so that we can determine whether we can keep this mapping in memory or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Number of distinct CODE-DESCRIPTION pairs:', df.select('CODE', 'DESCRIPTION').distinct().count())\n",
    "# print('Number of distinct CODEs:', df.select('CODE').distinct().count())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is discrepancy between the counts due to one `CODE` having 2 different descriptions.\n",
    "For simplification we simply choose one description over the other.\n",
    "\n",
    "Since there are only 159 diseases, we can perfectly keep the mapping in memory.\n",
    "And so, we collect the distinct `CODE`-`DESCRIPTION` pairs into an hash table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "code_description_map = {r.CODE: r.DESCRIPTION\n",
    "    for r in df \\\n",
    "    .select('CODE', 'DESCRIPTION') \\\n",
    "    .distinct() \\\n",
    "    .collect()\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the algorithm, the diseases' `DESCRIPTION`s won't be needed anymore, as we have the mapping.\n",
    "The distinct `PATIENT`-`CODE` pairs are taken (it doesn't make sense to have duplicate items within a basket)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('DESCRIPTION').distinct()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A-priori algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the support threshold parameter to 1000, as recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_threshold = 1000"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intermediate results of each pass are saved in disk in Parquet format (Spark's default format)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first pass, the frequent items are taken.\n",
    "For that, a \"group by\" operation is performed, grouping by `CODE` and counting the number of `PATIENTS` that each `CODE` is present in.\n",
    "Finally, the diseases are filtered according to the `support_threshold`, by comparing with the support stored in `COUNT`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('frequent_diseases_k1'):\n",
    "    frequent_diseases_k1 = df \\\n",
    "        .groupBy('CODE') \\\n",
    "        .count() \\\n",
    "        .withColumnRenamed('count', 'COUNT') \\\n",
    "        .filter(col('COUNT') >= support_threshold)\n",
    "    \n",
    "    frequent_diseases_k1.write.mode('overwrite').parquet(path='frequent_diseases_k1', compression='gzip')\n",
    "\n",
    "frequent_diseases_k1 = spark.read.parquet('frequent_diseases_k1')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The frequent items table is kept in memory for future passes (in a Python `set`, for quicker membership queries)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "frequent_diseases_k1_set = {r.CODE for r in frequent_diseases_k1.select('CODE').collect()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of frequent items: 131\n"
     ]
    }
   ],
   "source": [
    "print('Number of frequent items:', len(frequent_diseases_k1_set))   # 131"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second pass requires generating frequent pairs of items.\n",
    "For that, an UDF was developed that simply took an array of items and returned the list of item pairs, an operation performed within Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(returnType=ArrayType(ArrayType(StringType(), False), False))\n",
    "def combine_pairs(elems: Iterable[Any]):\n",
    "    return list(combinations(elems, 2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the `CODE`s are filtered using the `frequent_diseases_k1_set`, so that we only have frequent diseases (*monotonicity of itemsets*: itemsets are only frequent if all their subsets are).\n",
    "Then, for each `PATIENT` we collect its `CODE`s into an array, and then use that array in the UDF previously defined.\n",
    "\n",
    "It's important to note that the array of `CODE`s should be sorted beforehand, so that pair comparison can be done properly. Since Spark doesn't have a \"set\" datatype, the elements should be kept in order so that two pairs (which are arrays) with the same items will be considered equal.\n",
    "The `combinations` function is guaranteed to keep this order when generating pairs.\n",
    "\n",
    "The result is a column `CODE_PAIRS`, containing an array of pairs, being each pair an array with two elements.\n",
    "This column is exploded, producing a row for each pair within the arrays of pairs.\n",
    "\n",
    "Afterwards, the same grouping procedure in the first pass is performed, grouping by the itemsets and obtaining the number of baskets each itemset belongs to, filtering with the `support_threshold`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('frequent_diseases_k2'):\n",
    "    frequent_diseases_k2 = df \\\n",
    "        .filter(col('CODE').isin(frequent_diseases_k1_set)) \\\n",
    "        .groupBy('PATIENT') \\\n",
    "        .agg(collect_list('CODE')) \\\n",
    "        .withColumn('collect_list(CODE)', array_sort('collect_list(CODE)')) \\\n",
    "        .withColumn('CODE_PAIRS', combine_pairs('collect_list(CODE)')) \\\n",
    "        .select('PATIENT', 'CODE_PAIRS') \\\n",
    "        .withColumn('CODE_PAIR', explode('CODE_PAIRS')) \\\n",
    "        .drop('CODE_PAIRS') \\\n",
    "        .groupBy('CODE_PAIR') \\\n",
    "        .count() \\\n",
    "        .withColumnRenamed('count', 'COUNT') \\\n",
    "        .filter(col('COUNT') >= support_threshold)\n",
    "    \n",
    "    frequent_diseases_k2.write.mode('overwrite').parquet(path='frequent_diseases_k2', compression='gzip')\n",
    "\n",
    "frequent_diseases_k2 = spark.read.parquet('frequent_diseases_k2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table of frequent pairs is kept in memory for the third pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "frequent_diseases_k2_set = {tuple(r.CODE_PAIR) for r in frequent_diseases_k2.select('CODE_PAIR').collect()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of frequent pairs: 2940\n"
     ]
    }
   ],
   "source": [
    "print('Number of frequent pairs:', len(frequent_diseases_k2_set))   # 2940"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third Pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As was done for the second pass, an UDF was developed that returns an array of triples given an array of items.\n",
    "This function includes the verification that all $k-1$ immediate subsets of each returned triple are frequent (that is, all pairs within the triple are frequent)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(returnType=ArrayType(ArrayType(StringType(), False), False))\n",
    "def combine_triples(elems: Iterable[Any]):\n",
    "    return [\n",
    "        combination for combination in list(combinations(elems, 3))\n",
    "        if ((combination[0], combination[1]) in frequent_diseases_k2_set\n",
    "            and (combination[0], combination[2]) in frequent_diseases_k2_set\n",
    "            and (combination[1], combination[2]) in frequent_diseases_k2_set)\n",
    "    ]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same approach for the second pass was used, merely differing in the UDF used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('frequent_diseases_k3'):\n",
    "    frequent_diseases_k3 = df \\\n",
    "        .filter(col('CODE').isin(frequent_diseases_k1_set)) \\\n",
    "        .groupBy('PATIENT') \\\n",
    "        .agg(collect_list('CODE')) \\\n",
    "        .withColumn('collect_list(CODE)', array_sort('collect_list(CODE)')) \\\n",
    "        .withColumn('CODE_TRIPLES', combine_triples('collect_list(CODE)')) \\\n",
    "        .select('PATIENT', 'CODE_TRIPLES') \\\n",
    "        .withColumn('CODE_TRIPLE', explode('CODE_TRIPLES')) \\\n",
    "        .drop('CODE_TRIPLES') \\\n",
    "        .groupBy('CODE_TRIPLE') \\\n",
    "        .count() \\\n",
    "        .withColumnRenamed('count', 'COUNT') \\\n",
    "        .filter(col('COUNT') >= support_threshold)\n",
    "\n",
    "    frequent_diseases_k3.write.mode('overwrite').parquet(path='frequent_diseases_k3', compression='gzip')\n",
    "\n",
    "frequent_diseases_k3 = spark.read.parquet('frequent_diseases_k3')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table of frequent triples is generated, merely because the same was done for the previous $k$, but it won't be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "frequent_diseases_k3_set = {tuple(r.CODE_TRIPLE) for r in frequent_diseases_k3.select('CODE_TRIPLE').collect()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of frequent triples: 13392\n"
     ]
    }
   ],
   "source": [
    "print('Number of frequent triples:', len(frequent_diseases_k3_set))   # 13395"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most frequent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The listing of the 10 frequent pairs/triples is saved in a tab-separated CSV file, which includes the header.\n",
    "Obtaining the 10 most frequent itemsets involves sorting the respective dataframe in descending order and taking the top 10 results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "with open('most_frequent_k2.csv', 'w') as f:\n",
    "    print('pair\\tcount', file=f)\n",
    "    print(*(\n",
    "            f'{r.CODE_PAIR}\\t{r.COUNT}' for r in\n",
    "            frequent_diseases_k2.sort('COUNT', ascending=False).take(10)\n",
    "        ), sep='\\n', file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "with open('most_frequent_k3.csv', 'w') as f:\n",
    "    print('pair\\tcount', file=f)\n",
    "    print(*(\n",
    "            f'{r.CODE_TRIPLE}\\t{r.COUNT}' for r in\n",
    "            frequent_diseases_k3.sort('COUNT', ascending=False).take(10)\n",
    "        ), sep='\\n', file=f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Association Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total number of patients (baskets) is required for calculating the rule metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "n_patients = df.select('PATIENT').distinct().count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much like in the creation of pairs and triples, an UDF was created so that, taking an itemset as input, produces all subsets of items excluding the empty set and the complete set itself.\n",
    "\n",
    "Each of these subsets is associated with its complementary subset, with both being stored in a tuple.\n",
    "For instance, the subset $S'$ of itemset $S$ is associated with the subset $S''$ such that $S' \\cup S'' = S$.\n",
    "\n",
    "Therefore, essentially, the association rules are generated, where the head of the rule is the first subset and the tail is the complementary subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(returnType=ArrayType(ArrayType(ArrayType(StringType(), False), False), False))\n",
    "def inner_subsets(itemset: List[str]):\n",
    "    itemset = set(itemset)\n",
    "    combis = chain.from_iterable(list(map(set, combinations(itemset, k))) for k in range(1, len(itemset)))\n",
    "    return list((sorted(combi), sorted(itemset - combi)) for combi in combis)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate rules from the frequent pairs, the previous UDF is applied to the dataframe of item pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules_k2 = frequent_diseases_k2 \\\n",
    "    .withColumn('SUBSETS', inner_subsets('CODE_PAIR')) \\\n",
    "    .withColumn('SUBSETS', explode('SUBSETS')) \\\n",
    "    .withColumnRenamed('COUNT', 'COUNT_PAIR') \\\n",
    "    .select(col('CODE_PAIR')[0].alias('RULE_1'), col('CODE_PAIR')[1].alias('RULE_2'), 'COUNT_PAIR') \\\n",
    "    .join(frequent_diseases_k1, frequent_diseases_k1['CODE'] == col('RULE_1')[0], 'inner') \\\n",
    "    .withColumnRenamed('COUNT', 'COUNT_1') \\\n",
    "    .drop('CODE') \\\n",
    "    .join(frequent_diseases_k1, frequent_diseases_k1['CODE'] == col('RULE_2')[0], 'inner') \\\n",
    "    .withColumnRenamed('COUNT', 'COUNT_2') \\\n",
    "    .drop('CODE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules_k2_metrics = rules_k2 \\\n",
    "    .withColumn('CONFIDENCE', col('COUNT_PAIR') / col('COUNT_1')) \\\n",
    "    .withColumn('INTEREST', col('CONFIDENCE') - col('COUNT_2') / n_patients) \\\n",
    "    .withColumn('LIFT', n_patients * col('CONFIDENCE') / col('COUNT_2')) \\\n",
    "    .withColumn('STANDARDISED_LIFT', \n",
    "                (col('LIFT') - array_max(array(\n",
    "                    (col('COUNT_1') + col('COUNT_2')) / n_patients - 1,\n",
    "                    lit(1 / n_patients)\n",
    "                )) / (col('COUNT_1') * col('COUNT_2') / (n_patients ** 2)))\n",
    "                /\n",
    "                ((n_patients / array_max(array(col('COUNT_1'), col('COUNT_2')))) - array_max(array(\n",
    "                    (col('COUNT_1') + col('COUNT_2')) / n_patients - 1,\n",
    "                    lit(1 / n_patients)\n",
    "                )) / (col('COUNT_1') * col('COUNT_2') / (n_patients ** 2)))\n",
    "    ) \\\n",
    "    .filter(col('STANDARDISED_LIFT') >= 0.2) \\\n",
    "    .sort('STANDARDISED_LIFT', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules_k3 = frequent_diseases_k3 \\\n",
    "    .withColumn('CODE_TRIPLE', inner_subsets('CODE_TRIPLE')) \\\n",
    "    .withColumn('CODE_TRIPLE', explode('CODE_TRIPLE')) \\\n",
    "    .withColumnRenamed('COUNT', 'COUNT_TRIPLE') \\\n",
    "    .select(col('CODE_TRIPLE')[0].alias('RULE_1'), col('CODE_TRIPLE')[1].alias('RULE_2'), 'COUNT_TRIPLE') \\\n",
    "    \\\n",
    "    .join(frequent_diseases_k1, array(frequent_diseases_k1['CODE']) == col('RULE_1'), 'left') \\\n",
    "    .withColumnRenamed('COUNT', 'COUNT_1') \\\n",
    "    .drop('CODE') \\\n",
    "    .join(frequent_diseases_k2, frequent_diseases_k2['CODE_PAIR'] == col('RULE_1'), 'left') \\\n",
    "    .withColumnRenamed('COUNT', 'COUNT_1_OTHER') \\\n",
    "    .drop('CODE_PAIR') \\\n",
    "    .withColumn('COUNT_1', coalesce('COUNT_1', 'COUNT_1_OTHER')) \\\n",
    "    .drop('COUNT_1_OTHER') \\\n",
    "    \\\n",
    "    .join(frequent_diseases_k1, array(frequent_diseases_k1['CODE']) == col('RULE_2'), 'left') \\\n",
    "    .withColumnRenamed('COUNT', 'COUNT_2') \\\n",
    "    .drop('CODE') \\\n",
    "    .join(frequent_diseases_k2, frequent_diseases_k2['CODE_PAIR'] == col('RULE_2'), 'left') \\\n",
    "    .withColumnRenamed('COUNT', 'COUNT_2_OTHER') \\\n",
    "    .drop('CODE_PAIR') \\\n",
    "    .withColumn('COUNT_2', coalesce('COUNT_2', 'COUNT_2_OTHER')) \\\n",
    "    .drop('COUNT_2_OTHER')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules_k3_metrics = rules_k3 \\\n",
    "    .withColumn('CONFIDENCE', col('COUNT_TRIPLE') / col('COUNT_1')) \\\n",
    "    .withColumn('INTEREST', col('CONFIDENCE') - col('COUNT_2') / n_patients) \\\n",
    "    .withColumn('LIFT', n_patients * col('CONFIDENCE') / col('COUNT_2')) \\\n",
    "    .withColumn('STANDARDISED_LIFT', \n",
    "                (col('LIFT') - array_max(array(\n",
    "                    (col('COUNT_1') + col('COUNT_2')) / n_patients - 1,\n",
    "                    lit(1 / n_patients)\n",
    "                )) / (col('COUNT_1') * col('COUNT_2') / (n_patients ** 2)))\n",
    "                /\n",
    "                ((n_patients / array_max(array(col('COUNT_1'), col('COUNT_2')))) - array_max(array(\n",
    "                    (col('COUNT_1') + col('COUNT_2')) / n_patients - 1,\n",
    "                    lit(1 / n_patients)\n",
    "                )) / (col('COUNT_1') * col('COUNT_2') / (n_patients ** 2)))\n",
    "    ) \\\n",
    "    .filter(col('STANDARDISED_LIFT') >= 0.2) \\\n",
    "    .sort('STANDARDISED_LIFT', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 27:=========================================>                (5 + 2) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------+----------+-------+-------+-------------------+-------------------+------------------+------------------+\n",
      "|RULE_1          |RULE_2          |COUNT_PAIR|COUNT_1|COUNT_2|CONFIDENCE         |INTEREST           |LIFT              |STANDARDISED_LIFT |\n",
      "+----------------+----------------+----------+-------+-------+-------------------+-------------------+------------------+------------------+\n",
      "|[44054006]      |[422034002]     |20456     |77306  |20456  |0.26461076759889274|0.24693938821884232|14.973973559620212|1.0000000000000002|\n",
      "|[1551000119108] |[1501000119109] |3035      |11705  |3035   |0.25929090132422045|0.2566690477644603 |98.89602733874415 |1.0000000000000002|\n",
      "|[72892002]      |[398254007]     |22959     |205390 |22959  |0.11178246263206583|0.09194880995380139|5.635999805248552 |1.0000000000000002|\n",
      "|[44054006]      |[1551000119108] |11705     |77306  |11705  |0.1514112746746695 |0.14129964504798342|14.973973559620212|1.0000000000000002|\n",
      "|[67811000119102]|[254632001]     |2249      |2249   |2250   |1.0                |0.998056286487822  |514.4791111111111 |1.0               |\n",
      "|[128613002]     |[703151001]     |42693     |42693  |42693  |1.0                |0.9631186840109263 |27.11399995315391 |1.0               |\n",
      "|[47693006]      |[74400008]      |16103     |16103  |53933  |1.0                |0.9534087551767569 |21.463259970704392|1.0               |\n",
      "|[97331000119101]|[422034002]     |4291      |4291   |20456  |1.0                |0.9823286206199496 |56.58867813844349 |1.0               |\n",
      "|[198992004]     |[72892002]      |22738     |22738  |205390 |1.0                |0.8225691918816702 |5.635999805248551 |1.0               |\n",
      "|[713197008]     |[68496003]      |25903     |25903  |85587  |1.0                |0.9260637296147646 |13.525161531541006|1.0               |\n",
      "|[44054006]      |[431855005]     |32321     |77306  |32321  |0.4180917393216568 |0.3901705106873876 |14.97397355962021 |1.0               |\n",
      "|[72892002]      |[198992004]     |22738     |205390 |22738  |0.11070646087930279|0.09106372406156783|5.635999805248551 |1.0               |\n",
      "|[237602007]     |[302870006]     |74395     |74395  |75992  |1.0                |0.9343525879033637 |15.232892936098537|1.0               |\n",
      "|[6072007]       |[94260004]      |2049      |4422   |2049   |0.4633649932157395 |0.4615949181106494 |261.77702397105384|1.0               |\n",
      "|[302870006]     |[237602007]     |74395     |75992  |74395  |0.9789846299610485 |0.9147168225217226 |15.232892936098537|1.0               |\n",
      "|[254632001]     |[67811000119102]|2249      |2250   |2249   |0.9995555555555555 |0.9976127059160496 |514.4791111111111 |1.0               |\n",
      "|[703151001]     |[128613002]     |42693     |42693  |42693  |1.0                |0.9631186840109263 |27.11399995315391 |1.0               |\n",
      "|[64859006]      |[443165006]     |17268     |55301  |17268  |0.3122547512703206 |0.2973373979688584 |20.9323158713224  |1.0               |\n",
      "|[424132000]     |[162573006]     |12747     |12747  |15017  |1.0                |0.9870272240833879 |77.084504228541   |1.0               |\n",
      "|[72892002]      |[35999006]      |17873     |205390 |17873  |0.0870198159598812 |0.07157981969181115|5.635999805248551 |1.0               |\n",
      "+----------------+----------------+----------+-------+-------+-------------------+-------------------+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rules_k2_metrics.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(returnType=StringType())\n",
    "def format_rule(rule_1: List[str], rule_2: List[str], *values: List[Any]):\n",
    "    return f'{{{\", \".join(rule_1)}}} -> {{{\", \".join(rule_2)}}}: {\", \".join(map(str, values))}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "160"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_description_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 37:=========================================>                (5 + 2) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "|format_rule(RULE_1, RULE_2, STANDARDISED_LIFT, LIFT, CONFIDENCE, INTEREST)                                        |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "|{1551000119108} -> {1501000119109}: 1.0000000000000002, 98.89602733874415, 0.25929090132422045, 0.2566690477644603|\n",
      "|{44054006} -> {422034002}: 1.0000000000000002, 14.973973559620212, 0.26461076759889274, 0.24693938821884232       |\n",
      "|{44054006} -> {1551000119108}: 1.0000000000000002, 14.973973559620212, 0.1514112746746695, 0.14129964504798342    |\n",
      "|{72892002} -> {398254007}: 1.0000000000000002, 5.635999805248552, 0.11178246263206583, 0.09194880995380139        |\n",
      "|{443165006} -> {64859006}: 1.0, 20.9323158713224, 1.0, 0.9522269773613528                                         |\n",
      "|{64859006} -> {443165006}: 1.0, 20.9323158713224, 0.3122547512703206, 0.2973373979688584                          |\n",
      "|{1501000119109} -> {1551000119108}: 1.0, 98.89602733874413, 1.0, 0.9898883703733139                               |\n",
      "|{44054006} -> {431855005}: 1.0, 14.97397355962021, 0.4180917393216568, 0.3901705106873876                         |\n",
      "|{431855005} -> {44054006}: 1.0, 14.97397355962021, 1.0, 0.9332174592122517                                        |\n",
      "|{74400008} -> {47693006}: 1.0, 21.463259970704392, 0.2985741568242078, 0.2846632151857178                         |\n",
      "|{47693006} -> {74400008}: 1.0, 21.463259970704392, 1.0, 0.9534087551767569                                        |\n",
      "|{90781000119102} -> {431856006}: 1.0, 238.1357745319893, 0.9697593087842008, 0.9656870130080198                   |\n",
      "|{431856006} -> {90781000119102}: 1.0, 238.1357745319893, 1.0, 0.9958007149410234                                  |\n",
      "|{198992004} -> {72892002}: 1.0, 5.635999805248551, 1.0, 0.8225691918816702                                        |\n",
      "|{72892002} -> {198992004}: 1.0, 5.635999805248551, 0.11070646087930279, 0.09106372406156783                       |\n",
      "|{72892002} -> {35999006}: 1.0, 5.635999805248551, 0.0870198159598812, 0.07157981969181115                         |\n",
      "|{35999006} -> {72892002}: 1.0, 5.635999805248551, 1.0, 0.8225691918816702                                         |\n",
      "|{1551000119108} -> {302870006}: 1.0, 15.232892936098537, 1.0, 0.9343525879033637                                  |\n",
      "|{254637007} -> {162573006}: 1.0, 77.084504228541, 1.0, 0.9870272240833879                                         |\n",
      "|{162573006} -> {254637007}: 1.0, 77.084504228541, 0.8491043484051408, 0.83808910796346                            |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# TODO: map code to description\n",
    "\n",
    "rules_k2_metrics \\\n",
    "    .select(format_rule('RULE_1', 'RULE_2', 'STANDARDISED_LIFT', 'LIFT', 'CONFIDENCE', 'INTEREST')) \\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example output\n",
    "\n",
    "```\n",
    "...\n",
    "{Diabetes, Neoplasm} -> {Colon polyp}: 0.2000, ...\n",
    "...\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
