{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise the A-priori algorithm was developed to get the most frequent itemsets (size 2 and 3), and from them extract association rules. This algorithm was implemented using Spark, more specifically the PySpark library with the Dataframe API."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PySpark is the only non-standard library required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "from pyspark import Broadcast\n",
    "from pyspark.sql import SparkSession, DataFrame, Column\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StringType, ArrayType\n",
    "from itertools import combinations, chain\n",
    "from typing import Iterable, Any, List"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark initialization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark is initialized, with as many worker threads as logical cores on the machine.\n",
    "We did not use a fixed value since the machines used for development had a different number of CPU cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/03/18 10:26:20 WARN Utils: Your hostname, martinho-SATELLITE-L50-B resolves to a loopback address: 127.0.1.1; using 192.168.1.66 instead (on interface enp8s0)\n",
      "23/03/18 10:26:20 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/martinho/.local/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "23/03/18 10:26:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName('Apriori') \\\n",
    "    .config('spark.master', 'local[*]') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is medical data, where each row identifies a patient and their disease tested at a certain time.\n",
    "In this context, the diseases are the *items* and the patients are the *baskets*.\n",
    "\n",
    "The data's format is CSV, and is loaded including the header.\n",
    "The `START`, `STOP` and `ENCOUNTER` columns are removed as they are not useful for this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read \\\n",
    "    .option('header', True) \\\n",
    "    .csv('./data/conditions.csv.gz') \\\n",
    "    .drop('START', 'STOP', 'ENCOUNTER')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframe `df` will have three columns: `PATIENT`, `CODE` and `DESCRIPTION`.\n",
    "\n",
    "From this dataframe, we extract the mappings from `CODE` to its `DESCRIPTION`.\n",
    "Throughout the algorithm, we will use the `CODE` to identify each disease, and then map it to its `DESCRIPTION` when we output the final results.\n",
    "\n",
    "We first check how many unique diseases there are, so that we can determine whether we can keep this mapping in memory or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Number of distinct CODE-DESCRIPTION pairs:', df.select('CODE', 'DESCRIPTION').distinct().count())\n",
    "# print('Number of distinct CODEs:', df.select('CODE').distinct().count())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is discrepancy between the counts due to one `CODE` having 2 different descriptions.\n",
    "For simplification we simply choose one description over the other.\n",
    "\n",
    "Since there are only 159 diseases, we can perfectly keep the mapping in memory.\n",
    "And so, we collect the distinct `CODE`-`DESCRIPTION` pairs into an hash table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "code_description_map = {r.CODE: r.DESCRIPTION\n",
    "    for r in df \\\n",
    "    .select('CODE', 'DESCRIPTION') \\\n",
    "    .distinct() \\\n",
    "    .collect()\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the algorithm, the diseases' `DESCRIPTION`s won't be needed anymore, as we have the mapping.\n",
    "The distinct `PATIENT`-`CODE` pairs are taken (it doesn't make sense to have duplicate items within a basket)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('DESCRIPTION').distinct()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A-priori algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the support threshold parameter to 1000, as recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_threshold = 1000"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intermediate results of each pass are saved in disk in Parquet format (Spark's default format)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first pass, the frequent items are taken.\n",
    "For that, a \"group by\" operation is performed, grouping by `CODE` and counting the number of `PATIENTS` that each `CODE` is present in.\n",
    "Finally, the diseases are filtered according to the `support_threshold`, by comparing with the support stored in `COUNT`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('frequent_diseases_k1'):\n",
    "    frequent_diseases_k1 = df \\\n",
    "        .groupBy('CODE') \\\n",
    "        .count() \\\n",
    "        .withColumnRenamed('count', 'COUNT') \\\n",
    "        .filter(col('COUNT') >= support_threshold)\n",
    "    \n",
    "    frequent_diseases_k1.write.mode('overwrite').parquet(path='frequent_diseases_k1', compression='gzip')\n",
    "\n",
    "frequent_diseases_k1 = spark.read.parquet('frequent_diseases_k1')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The frequent items table is kept in memory for future passes (in a Python `set`, for quicker membership queries)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_diseases_k1_set = {r.CODE for r in frequent_diseases_k1.select('CODE').collect()}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this set of frequent items will be used multiple times by all nodes in the future, then we can broadcast this read-only data beforehand (https://spark.apache.org/docs/2.2.0/rdd-programming-guide.html#broadcast-variables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not isinstance(frequent_diseases_k1_set, Broadcast):\n",
    "    frequent_diseases_k1_set = spark.sparkContext.broadcast(frequent_diseases_k1_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of frequent items: 131\n"
     ]
    }
   ],
   "source": [
    "print('Number of frequent items:', len(frequent_diseases_k1_set.value))   # 131"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second pass requires generating frequent pairs of items.\n",
    "For that, an UDF was developed that simply took an array of items and returned the list of item pairs, an operation performed within Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(returnType=ArrayType(ArrayType(StringType(), False), False))\n",
    "def combine_pairs(elems: Iterable[Any]):\n",
    "    return list(combinations(elems, 2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the `CODE`s are filtered using the `frequent_diseases_k1_set`, so that we only have frequent diseases (*monotonicity of itemsets*: itemsets are only frequent if all their subsets are).\n",
    "Then, for each `PATIENT` we collect its `CODE`s into an array, and then use that array in the UDF previously defined.\n",
    "\n",
    "It's important to note that the array of `CODE`s should be sorted beforehand, so that pair comparison can be done properly. Since Spark doesn't have a \"set\" datatype, the elements should be kept in order so that two pairs (which are arrays) with the same items will be considered equal.\n",
    "The `combinations` function is guaranteed to keep this order when generating pairs.\n",
    "\n",
    "The result is a column `CODE_PAIRS`, containing an array of pairs, being each pair an array with two elements.\n",
    "This column is exploded, producing a row for each pair within the arrays of pairs.\n",
    "\n",
    "Afterwards, the same grouping procedure in the first pass is performed, grouping by the itemsets and obtaining the number of baskets each itemset belongs to, filtering with the `support_threshold`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('frequent_diseases_k2'):\n",
    "    frequent_diseases_k2 = df \\\n",
    "        .filter(col('CODE').isin(frequent_diseases_k1_set.value)) \\\n",
    "        .groupBy('PATIENT') \\\n",
    "        .agg(collect_list('CODE')) \\\n",
    "        .withColumn('collect_list(CODE)', array_sort('collect_list(CODE)')) \\\n",
    "        .withColumn('CODE_PAIRS', combine_pairs('collect_list(CODE)')) \\\n",
    "        .select('PATIENT', 'CODE_PAIRS') \\\n",
    "        .withColumn('CODE_PAIR', explode('CODE_PAIRS')) \\\n",
    "        .drop('CODE_PAIRS') \\\n",
    "        .groupBy('CODE_PAIR') \\\n",
    "        .count() \\\n",
    "        .withColumnRenamed('count', 'COUNT') \\\n",
    "        .filter(col('COUNT') >= support_threshold)\n",
    "    \n",
    "    frequent_diseases_k2.write.mode('overwrite').parquet(path='frequent_diseases_k2', compression='gzip')\n",
    "\n",
    "frequent_diseases_k2 = spark.read.parquet('frequent_diseases_k2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table of frequent pairs is kept in memory for the third pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "frequent_diseases_k2_set = {tuple(r.CODE_PAIR) for r in frequent_diseases_k2.select('CODE_PAIR').collect()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of frequent pairs: 2940\n"
     ]
    }
   ],
   "source": [
    "print('Number of frequent pairs:', len(frequent_diseases_k2_set))   # 2940"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third Pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As was done for the second pass, an UDF was developed that returns an array of triples given an array of items.\n",
    "This function includes the verification that all $k-1$ immediate subsets of each returned triple are frequent (that is, all pairs within the triple are frequent)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(returnType=ArrayType(ArrayType(StringType(), False), False))\n",
    "def combine_triples(elems: Iterable[Any]):\n",
    "    return [\n",
    "        combination for combination in list(combinations(elems, 3))\n",
    "        if ((combination[0], combination[1]) in frequent_diseases_k2_set\n",
    "            and (combination[0], combination[2]) in frequent_diseases_k2_set\n",
    "            and (combination[1], combination[2]) in frequent_diseases_k2_set)\n",
    "    ]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same approach for the second pass was used, merely differing in the UDF used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('frequent_diseases_k3'):\n",
    "    frequent_diseases_k3 = df \\\n",
    "        .filter(col('CODE').isin(frequent_diseases_k1_set.value)) \\\n",
    "        .groupBy('PATIENT') \\\n",
    "        .agg(collect_list('CODE')) \\\n",
    "        .withColumn('collect_list(CODE)', array_sort('collect_list(CODE)')) \\\n",
    "        .withColumn('CODE_TRIPLES', combine_triples('collect_list(CODE)')) \\\n",
    "        .select('PATIENT', 'CODE_TRIPLES') \\\n",
    "        .withColumn('CODE_TRIPLE', explode('CODE_TRIPLES')) \\\n",
    "        .drop('CODE_TRIPLES') \\\n",
    "        .groupBy('CODE_TRIPLE') \\\n",
    "        .count() \\\n",
    "        .withColumnRenamed('count', 'COUNT') \\\n",
    "        .filter(col('COUNT') >= support_threshold)\n",
    "\n",
    "    frequent_diseases_k3.write.mode('overwrite').parquet(path='frequent_diseases_k3', compression='gzip')\n",
    "\n",
    "frequent_diseases_k3 = spark.read.parquet('frequent_diseases_k3')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table of frequent triples is generated, merely because the same was done for the previous $k$, but it won't be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "frequent_diseases_k3_set = {tuple(r.CODE_TRIPLE) for r in frequent_diseases_k3.select('CODE_TRIPLE').collect()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of frequent triples: 13386\n"
     ]
    }
   ],
   "source": [
    "print('Number of frequent triples:', len(frequent_diseases_k3_set))   # 13395"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most frequent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For outputting the most frequent itemsets, an UDF was developed merely so the disease `CODE`s are converted to their descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(returnType=ArrayType(StringType(), False))\n",
    "def map_codes_to_description(codes: List[str]):\n",
    "    return [code_description_map[item] for item in codes]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The listing of the 10 frequent pairs/triples is saved in a tab-separated CSV file, which includes the header.\n",
    "Obtaining the 10 most frequent itemsets involves sorting the respective dataframe in descending order and taking the top 10 results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "with open('most_frequent_k2.csv', 'w') as f:\n",
    "    print('pair\\tcount', file=f)\n",
    "    print(*(\n",
    "            f'{r.CODE_PAIR}\\t{r.COUNT}' for r in\n",
    "            frequent_diseases_k2\n",
    "                .withColumn('CODE_PAIR', map_codes_to_description('CODE_PAIR'))\n",
    "                .sort('COUNT', ascending=False).take(10)\n",
    "        ), sep='\\n', file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "with open('most_frequent_k3.csv', 'w') as f:\n",
    "    print('triple\\tcount', file=f)\n",
    "    print(*(\n",
    "            f'{r.CODE_TRIPLE}\\t{r.COUNT}' for r in\n",
    "            frequent_diseases_k3\n",
    "                .withColumn('CODE_TRIPLE', map_codes_to_description('CODE_TRIPLE'))\n",
    "                .sort('COUNT', ascending=False).take(10)\n",
    "        ), sep='\\n', file=f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Association Rules"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout this section, for a rule $A \\rightarrow B$ we denote $A$ (the LHS) as the **head** and $B$ (the RHS) as the **tail**.\n",
    "\n",
    "The total number of patients (baskets) is required for calculating the rule metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "n_patients = df.select('PATIENT').distinct().count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much like in the creation of pairs and triples, an UDF was created so that, taking an itemset as input, produces all subsets of items excluding the empty set and the complete set itself.\n",
    "\n",
    "Each of these subsets is associated with its \"complementary\" subset, with both being stored in a tuple.\n",
    "For instance, the subset $S_1$ of itemset $S$ is associated with the subset $S_2$ such that $S_1 \\cup S_2 = S$ and $S_1 \\cap S_2 = \\emptyset$.\n",
    "\n",
    "Therefore, essentially, the association rules are generated, where the head of the rule is the first subset and the tail is its \"complementary\" subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(returnType=ArrayType(ArrayType(ArrayType(StringType(), False), False), False))\n",
    "def generate_association_rules(itemset: List[str]):\n",
    "    itemset = set(itemset)\n",
    "    combis = chain.from_iterable(list(map(set, combinations(itemset, k))) for k in range(1, len(itemset)))\n",
    "    return list((sorted(combi), sorted(itemset - combi)) for combi in combis)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate rules from the frequent pairs, the previous UDF is applied to the dataframe of item pairs (`frequent_diseases_k2`).\n",
    "The head and tail of the generated subsets is put in two separate columns, `RULE_HEAD` and `RULE_TAIL` respectively.\n",
    "The support of pairs is kept in `COUNT_PAIR` to be used when calculating the metrics.\n",
    "\n",
    "Afterwards, the dataframe is joined with the dataframe containing the frequent items and their support (`frequent_diseases_k1`) on the disease `CODE`s, so that we can obtain the support of the rule's head and tail.\n",
    "And so, two inner joins are performed, since there it's guaranteed that the rule head/tail is present in the dataframe of frequent items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules_k2 = frequent_diseases_k2 \\\n",
    "    .withColumn('RULES', generate_association_rules('CODE_PAIR')) \\\n",
    "    .withColumn('RULES', explode('RULES')) \\\n",
    "    .select(col('RULES')[0].alias('RULE_HEAD'), col('RULES')[1].alias('RULE_TAIL'), col('COUNT').alias('COUNT_RULE')) \\\n",
    "    .join(frequent_diseases_k1, frequent_diseases_k1['CODE'] == col('RULE_HEAD')[0], 'inner') \\\n",
    "    .withColumnRenamed('COUNT', 'COUNT_HEAD') \\\n",
    "    .drop('CODE') \\\n",
    "    .join(frequent_diseases_k1, frequent_diseases_k1['CODE'] == col('RULE_TAIL')[0], 'inner') \\\n",
    "    .withColumnRenamed('COUNT', 'COUNT_TAIL') \\\n",
    "    .drop('CODE')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the necessary counts on separate columns, the metrics can be easily calculated by referencing the column containing the desired values.\n",
    "The `CONFIDENCE` column is created first because the result of this calculation is used to calculate the other metrics.\n",
    "The results are finally filtered according to the `STANDARDISED_LIFT`.\n",
    "\n",
    "The calculation of metrics is put in a separate function since it will be reused for the rules extracted from triples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_metrics_columns(rule_counts: DataFrame) -> DataFrame:\n",
    "    return rule_counts \\\n",
    "        .withColumn('CONFIDENCE', col('COUNT_RULE') / col('COUNT_HEAD')) \\\n",
    "        .withColumn('INTEREST', col('CONFIDENCE') - col('COUNT_TAIL') / n_patients) \\\n",
    "        .withColumn('LIFT', n_patients * col('CONFIDENCE') / col('COUNT_TAIL')) \\\n",
    "        .withColumn('STANDARDISED_LIFT', \n",
    "                    (col('LIFT') - array_max(array(\n",
    "                        (col('COUNT_HEAD') + col('COUNT_TAIL')) / n_patients - 1,\n",
    "                        lit(1 / n_patients)\n",
    "                    )) / (col('COUNT_HEAD') * col('COUNT_TAIL') / (n_patients ** 2)))\n",
    "                    /\n",
    "                    ((n_patients / array_max(array(col('COUNT_HEAD'), col('COUNT_TAIL')))) - array_max(array(\n",
    "                        (col('COUNT_HEAD') + col('COUNT_TAIL')) / n_patients - 1,\n",
    "                        lit(1 / n_patients)\n",
    "                    )) / (col('COUNT_HEAD') * col('COUNT_TAIL') / (n_patients ** 2)))\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules_k2_metrics = rules_k2 \\\n",
    "    .transform(add_metrics_columns) \\\n",
    "    .filter(col('STANDARDISED_LIFT') >= 0.2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the rules extracted from the frequent triples, a similar approach to the frequent pairs was used, on the dataframe of frequent triples (`frequent_diseases_k3`).\n",
    "\n",
    "However, the joins had to be performed differently.\n",
    "Since the rules' head/tail can be a subset of different sizes (in this case, it can have one or two elements), the joins have to be performed both with the dataframe of frequent items (`frequent_diseases_k1`) and the dataframe of frequent pairs (`frequent_diseases_k2`).\n",
    "\n",
    "Because of this, the join can't be *inner*, and instead should be a *left join*, so that non-matching `CODE`s aren't removed, but kept with a `null` value on `COUNT`.\n",
    "To obtain the support of the rules' head for instance, we join with `frequent_diseases_k1`, and so the rule heads that have one element will have a count while the rule heads that have two elements will have a `null` value (`COUNT_HEAD`).\n",
    "Afterwards, joining with `frequent_diseases_k2` provides the counts for rule heads that have two elements, while heads with one element have a `null` value (`COUNT_HEAD_OTHER`).\n",
    "\n",
    "Both columns `COUNT_HEAD` and `COUNT_HEAD_OTHER` are \"complementary\" to each other, that is, for a given row a column has a non-`null` value while the other has `null`.\n",
    "In order to combine both into a single column `COUNT_HEAD`, `coalesce` is used which uses the first non-`null` value of both columns for all rows.\n",
    "\n",
    "Since this operation is lengthy in lines of code and is done twice (for the heads and tails), it was extracted to a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_counts_columns_to_rules_from_triples(dataframe_with_rules_from_triples: DataFrame, join_key: Column, counts_column_name: str) -> DataFrame:\n",
    "    counts_column_name_other = counts_column_name + '_OTHER'\n",
    "    return dataframe_with_rules_from_triples \\\n",
    "        .join(frequent_diseases_k1, array(frequent_diseases_k1['CODE']) == join_key, 'left') \\\n",
    "        .withColumnRenamed('COUNT', counts_column_name) \\\n",
    "        .drop('CODE') \\\n",
    "        .join(frequent_diseases_k2, frequent_diseases_k2['CODE_PAIR'] == join_key, 'left') \\\n",
    "        .withColumnRenamed('COUNT', counts_column_name_other) \\\n",
    "        .drop('CODE_PAIR') \\\n",
    "        .withColumn(counts_column_name, coalesce(counts_column_name, counts_column_name_other)) \\\n",
    "        .drop(counts_column_name_other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules_k3 = frequent_diseases_k3 \\\n",
    "    .withColumn('RULES', generate_association_rules('CODE_TRIPLE')) \\\n",
    "    .withColumn('RULES', explode('RULES')) \\\n",
    "    .select(col('RULES')[0].alias('RULE_HEAD'), col('RULES')[1].alias('RULE_TAIL'), col('COUNT').alias('COUNT_RULE')) \\\n",
    "    .transform(add_counts_columns_to_rules_from_triples, join_key=col('RULE_HEAD'), counts_column_name='COUNT_HEAD') \\\n",
    "    .transform(add_counts_columns_to_rules_from_triples, join_key=col('RULE_TAIL'), counts_column_name='COUNT_TAIL')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the counts obtained, calculating the metrics is exactly the same to the rule metrics obtained from the pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules_k3_metrics = rules_k3 \\\n",
    "    .transform(add_metrics_columns) \\\n",
    "    .filter(col('STANDARDISED_LIFT') >= 0.2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, both dataframes `rules_k2_metrics` and `rules_k3_metrics` are merged into a dataframe containing all their rows.\n",
    "After this, we can sort by the `STANDARDISED_LIFT` for printing the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert rules_k2_metrics.columns == rules_k3_metrics.columns, 'The dataframes of rule metrics should have the same columns in the same order!'\n",
    "\n",
    "rules_metrics = rules_k2_metrics \\\n",
    "    .union(rules_k3_metrics) \\\n",
    "    .sort('STANDARDISED_LIFT', ascending=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate the final results, the disease codes are mapped to their respective descriptions and the columns are transformed into a single `String` column.\n",
    "\n",
    "Then, the dataframe is written to disk, into a partitioned text file.\n",
    "If the file is to be read, then the following command can be used (if on Linux):\n",
    "\n",
    "```bash\n",
    "cat association_rules/part* | less\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(returnType=StringType())\n",
    "def format_rule(rule_1: List[str], rule_2: List[str], *values: List[Any]):\n",
    "    return f'{{{\", \".join(rule_1)}}} -> {{{\", \".join(rule_2)}}}: {\", \".join(map(str, values))}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rules_metrics \\\n",
    "    .withColumn('RULE_HEAD', map_codes_to_description('RULE_HEAD')) \\\n",
    "    .withColumn('RULE_TAIL', map_codes_to_description('RULE_TAIL')) \\\n",
    "    .select(format_rule('RULE_HEAD', 'RULE_TAIL', 'STANDARDISED_LIFT', 'LIFT', 'CONFIDENCE', 'INTEREST').alias('LINE')) \\\n",
    "    .write.text('association_rules')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
