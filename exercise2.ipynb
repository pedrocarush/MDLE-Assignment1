{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import random\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession, Row, DataFrame\n",
    "from pyspark.sql.types import StringType, ArrayType, IntegerType, StructType, StructField, DecimalType\n",
    "from itertools import combinations, chain\n",
    "from functools import partial\n",
    "from typing import Iterable, Any, List, Callable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values for parameters $b$ and $r$ chosen, according to the requirements, were:\n",
    "- $b = 13$\n",
    "- $r = 11$\n",
    "\n",
    "The values were hand-picked by visually analyzing the plot for the probability of two documents sharing a bucket depending on their similarity, as $b$ and $r$ changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "N = 100\n",
    "\n",
    "r = 11\n",
    "b = 13\n",
    "\n",
    "point_below = (0.85, 0.9)\n",
    "point_above = (0.6, 0.05)\n",
    "\n",
    "prob = lambda s, r, b: 1 - (1 - s**r)**b\n",
    "\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    ss = [i/N for i in range(N)]\n",
    "\n",
    "    plt.plot(*point_below, color='g', marker='o')\n",
    "    plt.plot(*point_above, color='r', marker='o')\n",
    "    plt.plot(ss, [prob(s, r, b) for s in ss])\n",
    "\n",
    "    plt.title(f'Probability of two documents sharing a bucket w.r.t. their similarity $s$\\n($r={r}$, $b={b}$)')\n",
    "    plt.legend(['at least', 'less than', 'probability'])\n",
    "    plt.xlabel('$s$')\n",
    "    plt.ylabel('probability')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "except ImportError:\n",
    "    print('Could not plot, since the \\'matplotlib\\' module is not present.')\n",
    "\n",
    "assert prob(point_below[0], r, b) >= point_below[1], 'Pairs with a similarity of 85%% should have at least 90%% probability of sharing a bucket!'\n",
    "assert prob(point_above[0], r, b) <  point_above[1], 'Pairs with a similarity of 60%% should have less than 5%% probability of sharing a bucket!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shingle size\n",
    "k = 9\n",
    "\n",
    "# Number of bands\n",
    "b = 13\n",
    "\n",
    "# Number of rows per band\n",
    "r = 11\n",
    "\n",
    "# Min-hash: number of hash functions\n",
    "num_functions = b*r\n",
    "\n",
    "# Seed for the random number generator\n",
    "seed = 123\n",
    "\n",
    "# Similarity threshold\n",
    "similarity_threshold = 0.85\n",
    "\n",
    "# Sample of the dataset to use\n",
    "sample_fraction = 0.01\n",
    "\n",
    "# Number of explicit partitions\n",
    "num_partitions = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(seed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/22 14:24:47 WARN Utils: Your hostname, martinho-SATELLITE-L50-B resolves to a loopback address: 127.0.1.1; using 192.168.47.76 instead (on interface wlx200db038271f)\n",
      "23/03/22 14:24:47 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/22 14:24:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName('LSH') \\\n",
    "    .config('spark.master', 'local[*]') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# TODO: Configure partitions for speedup?\n",
    "df = spark.read \\\n",
    "    .json('./data/covid_news_small.json.bz2') \\\n",
    "    .repartition(num_partitions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate shingles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: ignore punctuation? use different shingling strategy? (see 'Further fun' slide, which is the last, of 3b)\n",
    "@F.udf(returnType=ArrayType(IntegerType(), False))\n",
    "def generate_shingles(text: str):\n",
    "    shingles = (text[idx:idx+k] for idx in range(len(text) - k + 1))\n",
    "    # Get last 32 bits in order to have 4-byte integers (Python allows arbitrarily large integers)\n",
    "    to_integer = lambda s: hash(s) & ((1 << 32) - 1)\n",
    "    return list(set(to_integer(shingle_str) for shingle_str in shingles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shingles = df \\\n",
    "    .drop('url') \\\n",
    "    .filter(F.length('text') >= k) \\\n",
    "    .repartition(num_partitions) \\\n",
    "    .withColumn('shingles', generate_shingles('text')) \\\n",
    "    .drop('text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Min-hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assumes the values to hash are 4-byte integers\n",
    "def generate_universal_hash_family(K: int) -> List[Callable[[int], int]]:\n",
    "    N = 1 << 32\n",
    "    p = 2305843009213693951\n",
    "\n",
    "    parameters = set()\n",
    "    while (len(parameters) < K):\n",
    "        parameters |= {(random.randint(1, N), random.randint(0, N)) for _ in range(K - len(parameters))}\n",
    "    \n",
    "    return [(a, b, p, N) for a, b in parameters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_family = generate_universal_hash_family(num_functions)\n",
    "broadcasted_hash_family = spark.sparkContext.broadcast(hash_family)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@F.udf(returnType=ArrayType(IntegerType(), False))\n",
    "def calculate_min_hash(shingles: List[int]):\n",
    "    return [min(((a * shingle + b) % p) % N for shingle in shingles) for (a, b, p, N) in broadcasted_hash_family.value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_minhash = df_shingles.withColumn('min_hash', calculate_min_hash('shingles')).drop('shingles')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#t = time.time()\n",
    "#hash_col_names = [f'hashed_{i}' for i in range(num_functions)]\n",
    "\n",
    "#data = df_shingles \\\n",
    "#    .withColumn('shingles', F.explode('shingles')) \\\n",
    "#    .withColumnRenamed('shingles', 'shingle') \\\n",
    "#    .select('tweet_id', *( (((a * F.col('shingle').cast(DecimalType()) + b) % p) % N).alias(name) for (a, b, p, N), name in zip(hash_family, hash_col_names) )) \\\n",
    "#    .groupby('tweet_id') \\\n",
    "#    .min(*hash_col_names) \\\n",
    "#    .withColumn('min_hash', F.array(*(f'min({name})' for name in hash_col_names))) \\\n",
    "#    .select('tweet_id', 'min_hash') \\\n",
    "#    .collect()\n",
    "\n",
    "#print('Execution time:', time.time() - t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = time.time()\n",
    "# N = 1 << 32\n",
    "# p = 2305843009213693951\n",
    "\n",
    "# data2 = df_shingles \\\n",
    "#     .withColumn('shingles', F.explode('shingles')) \\\n",
    "#     .withColumnRenamed('shingles', 'shingle') \\\n",
    "#     .withColumn('hash_params', F.explode(F.array(*(F.struct(F.lit(a).alias('a'), F.lit(b).alias('b')) for a, b, p, N in hash_family)))) \\\n",
    "#     .withColumn('shingle', ((F.col('hash_params').a * F.col('shingle') + F.col('hash_params').b) % p) % N) \\\n",
    "#     .groupby('tweet_id', 'hash_params') \\\n",
    "#     .min('shingle') \\\n",
    "#     .collect()\n",
    "\n",
    "# print('Execution time:', time.time() - t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#t = time.time()\n",
    "\n",
    "#data3 = df_minhash.collect()\n",
    "\n",
    "#print('Execution time:', time.time() - t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data[0].min_hash\n",
    "\n",
    "#lista = [x for x in data3 if x.tweet_id == '1436355959601913862']\n",
    "\n",
    "#lista[0].min_hash\n",
    "\n",
    "#len(lista)\n",
    "\n",
    "#display(sorted(data[0].min_hash))\n",
    "#display(sorted(lista[0].min_hash))\n",
    "#set(lista[0].min_hash) == set(data[0].min_hash)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "fname_minhash = f'minhash_{r}_{b}'\n",
    "if not os.path.exists(fname_minhash):\n",
    "    df_minhash.write.mode('overwrite').parquet(path=fname_minhash, compression='gzip')\n",
    "\n",
    "df_minhash = spark.read.parquet(fname_minhash)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@F.udf(returnType=ArrayType(ArrayType(IntegerType(), False), False))\n",
    "def generate_even_slices(minhashes: List[int]):\n",
    "    return [minhashes[i:i+r] for i in range(0, num_functions, r)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_bands(df: DataFrame) -> DataFrame:\n",
    "    return df \\\n",
    "        .withColumn('min_hash_slices', generate_even_slices('min_hash')) \\\n",
    "        .withColumn('bands', F.array(*(\n",
    "            F.struct(\n",
    "                F.hash(F.col('min_hash_slices')[band]).alias('band_hash'),\n",
    "                F.lit(band).alias('band')\n",
    "            )\n",
    "            for band in range(b))\n",
    "        )) \\\n",
    "        .withColumn('bands', F.explode('bands')) \\\n",
    "        .select('tweet_id', F.col('bands').band.alias('band'), F.col('bands').band_hash.alias('band_hash'))\n",
    "\n",
    "df_bands = create_df_bands(df_minhash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@F.udf(returnType=ArrayType(ArrayType(StringType(), False), False))\n",
    "def combine_pairs(elems: Iterable[Any]):\n",
    "    return list(combinations(elems, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_candidate_pairs(df: DataFrame) -> DataFrame:\n",
    "    return df \\\n",
    "        .groupby('band', 'band_hash') \\\n",
    "        .agg(F.collect_list('tweet_id')) \\\n",
    "        .withColumnRenamed('collect_list(tweet_id)', 'candidates') \\\n",
    "        .withColumn('candidates', F.array_sort('candidates')) \\\n",
    "        .select(F.explode(combine_pairs('candidates')).alias('candidate_pair')) \\\n",
    "        .select(F.col('candidate_pair')[0].alias('candidate_pair_first'), F.col('candidate_pair')[1].alias('candidate_pair_second'))\n",
    "\n",
    "df_candidate_pairs = create_candidate_pairs(df_bands)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove false positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_candidate_pairs_fpless(df_candidate_pairs: DataFrame, df_minhash: DataFrame, similarity_threshold: float) -> DataFrame:\n",
    "    return df_candidate_pairs \\\n",
    "        .join(df_minhash, df_minhash['tweet_id'] == F.col('candidate_pair_first')) \\\n",
    "        .withColumnRenamed('min_hash', 'min_hash_first') \\\n",
    "        .drop('tweet_id') \\\n",
    "        .join(df_minhash, df_minhash['tweet_id'] == F.col('candidate_pair_second')) \\\n",
    "        .withColumnRenamed('min_hash', 'min_hash_second') \\\n",
    "        .drop('tweet_id') \\\n",
    "        .withColumn('similarity', F.size(F.array_intersect('min_hash_first', 'min_hash_second')) / F.size(F.array_union('min_hash_first', 'min_hash_second'))) \\\n",
    "        .filter(F.col('similarity') >= similarity_threshold)\n",
    "\n",
    "df_candidate_pairs_fpless = create_df_candidate_pairs_fpless(df_candidate_pairs, df_minhash, similarity_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:>                                                          (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/22 14:31:47 ERROR Utils: Aborting task\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.util.Arrays.copyOf(Arrays.java:3689)\n",
      "\tat java.base/java.util.ArrayList.grow(ArrayList.java:238)\n",
      "\tat java.base/java.util.ArrayList.addAll(ArrayList.java:710)\n",
      "\tat net.razorvine.pickle.Unpickler.load_appends(Unpickler.java:726)\n",
      "\tat net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:232)\n",
      "\tat net.razorvine.pickle.Unpickler.load(Unpickler.java:109)\n",
      "\tat net.razorvine.pickle.Unpickler.loads(Unpickler.java:122)\n",
      "\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.$anonfun$evaluate$6(BatchEvalPythonExec.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$Lambda$3854/0x00000001015e3840.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:341)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$$Lambda$4204/0x000000010183fc40.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:348)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$22(FileFormatWriter.scala:266)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$$Lambda$4092/0x00000001017e4040.apply(Unknown Source)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2398/0x0000000101119c40.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/martinho/.local/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/martinho/.local/lib/python3.8/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/usr/lib/python3.8/socket.py\", line 669, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/martinho/ua/4-2/mdle/assign1/exercise2.ipynb Cell 37\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/martinho/ua/4-2/mdle/assign1/exercise2.ipynb#X51sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m fname_candidate_pairs \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcandidate_pairs_\u001b[39m\u001b[39m{\u001b[39;00mr\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mb\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/martinho/ua/4-2/mdle/assign1/exercise2.ipynb#X51sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(fname_candidate_pairs):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/martinho/ua/4-2/mdle/assign1/exercise2.ipynb#X51sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     df_candidate_pairs_fpless\u001b[39m.\u001b[39;49mwrite\u001b[39m.\u001b[39;49mmode(\u001b[39m'\u001b[39;49m\u001b[39moverwrite\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39;49mparquet(path\u001b[39m=\u001b[39;49mfname_candidate_pairs, compression\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mgzip\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/martinho/ua/4-2/mdle/assign1/exercise2.ipynb#X51sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m df_candidate_pairs_fpless \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39mread\u001b[39m.\u001b[39mparquet(fname_candidate_pairs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/readwriter.py:1140\u001b[0m, in \u001b[0;36mDataFrameWriter.parquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpartitionBy(partitionBy)\n\u001b[1;32m   1139\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_opts(compression\u001b[39m=\u001b[39mcompression)\n\u001b[0;32m-> 1140\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jwrite\u001b[39m.\u001b[39;49mparquet(path)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/java_gateway.py:1320\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1313\u001b[0m args_command, temp_args \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_args(\u001b[39m*\u001b[39margs)\n\u001b[1;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client\u001b[39m.\u001b[39;49msend_command(command)\n\u001b[1;32m   1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_id, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname)\n\u001b[1;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49msend_command(command)\n\u001b[1;32m   1039\u001b[0m     \u001b[39mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[39mreturn\u001b[39;00m response, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[39m=\u001b[39m smart_decode(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstream\u001b[39m.\u001b[39;49mreadline()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mAnswer received: \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[39m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[39m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.8/socket.py:669\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    668\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 669\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    670\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    671\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:>                                                          (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/22 14:34:51 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.rpc.RpcTimeoutException: Futures timed out after [10000 milliseconds]. This timeout is controlled by spark.executor.heartbeatInterval\n",
      "\tat org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:76)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1053)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:238)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "Caused by: java.util.concurrent.TimeoutException: Futures timed out after [10000 milliseconds]\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:259)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:263)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:293)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\t... 12 more\n",
      "23/03/22 14:34:52 WARN NettyRpcEnv: Ignored message: HeartbeatResponse(false)\n",
      "23/03/22 14:35:07 ERROR FileFormatWriter: Job job_202303221426274238332233299830246_0006 aborted.\n",
      "23/03/22 14:35:08 ERROR Executor: Exception in task 1.0 in stage 6.0 (TID 13)\n",
      "org.apache.spark.SparkException: Task failed while writing rows.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:655)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:358)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$22(FileFormatWriter.scala:266)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "Caused by: java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.util.Arrays.copyOf(Arrays.java:3689)\n",
      "\tat java.base/java.util.ArrayList.grow(ArrayList.java:238)\n",
      "\tat java.base/java.util.ArrayList.addAll(ArrayList.java:710)\n",
      "\tat net.razorvine.pickle.Unpickler.load_appends(Unpickler.java:726)\n",
      "\tat net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:232)\n",
      "\tat net.razorvine.pickle.Unpickler.load(Unpickler.java:109)\n",
      "\tat net.razorvine.pickle.Unpickler.loads(Unpickler.java:122)\n",
      "\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.$anonfun$evaluate$6(BatchEvalPythonExec.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$Lambda$3854/0x00000001015e3840.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:341)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$$Lambda$4204/0x000000010183fc40.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:348)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$22(FileFormatWriter.scala:266)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$$Lambda$4092/0x00000001017e4040.apply(Unknown Source)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2398/0x0000000101119c40.apply(Unknown Source)\n",
      "\t... 5 more\n",
      "23/03/22 14:35:08 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 1.0 in stage 6.0 (TID 13),5,main]\n",
      "org.apache.spark.SparkException: Task failed while writing rows.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:655)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:358)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$22(FileFormatWriter.scala:266)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "Caused by: java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.util.Arrays.copyOf(Arrays.java:3689)\n",
      "\tat java.base/java.util.ArrayList.grow(ArrayList.java:238)\n",
      "\tat java.base/java.util.ArrayList.addAll(ArrayList.java:710)\n",
      "\tat net.razorvine.pickle.Unpickler.load_appends(Unpickler.java:726)\n",
      "\tat net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:232)\n",
      "\tat net.razorvine.pickle.Unpickler.load(Unpickler.java:109)\n",
      "\tat net.razorvine.pickle.Unpickler.loads(Unpickler.java:122)\n",
      "\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.$anonfun$evaluate$6(BatchEvalPythonExec.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$Lambda$3854/0x00000001015e3840.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:341)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$$Lambda$4204/0x000000010183fc40.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:348)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$22(FileFormatWriter.scala:266)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$$Lambda$4092/0x00000001017e4040.apply(Unknown Source)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2398/0x0000000101119c40.apply(Unknown Source)\n",
      "\t... 5 more\n",
      "23/03/22 14:35:08 WARN TaskSetManager: Lost task 1.0 in stage 6.0 (TID 13) (aluno-3886.wireless.ua.pt executor driver): org.apache.spark.SparkException: Task failed while writing rows.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:655)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:358)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$22(FileFormatWriter.scala:266)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "Caused by: java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.util.Arrays.copyOf(Arrays.java:3689)\n",
      "\tat java.base/java.util.ArrayList.grow(ArrayList.java:238)\n",
      "\tat java.base/java.util.ArrayList.addAll(ArrayList.java:710)\n",
      "\tat net.razorvine.pickle.Unpickler.load_appends(Unpickler.java:726)\n",
      "\tat net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:232)\n",
      "\tat net.razorvine.pickle.Unpickler.load(Unpickler.java:109)\n",
      "\tat net.razorvine.pickle.Unpickler.loads(Unpickler.java:122)\n",
      "\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.$anonfun$evaluate$6(BatchEvalPythonExec.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$Lambda$3854/0x00000001015e3840.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:341)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$$Lambda$4204/0x000000010183fc40.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:348)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$22(FileFormatWriter.scala:266)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$$Lambda$4092/0x00000001017e4040.apply(Unknown Source)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2398/0x0000000101119c40.apply(Unknown Source)\n",
      "\t... 5 more\n",
      "\n",
      "23/03/22 14:35:08 ERROR TaskSetManager: Task 1 in stage 6.0 failed 1 times; aborting job\n",
      "23/03/22 14:35:09 ERROR FileFormatWriter: Aborting job 2433f78a-7612-4b16-8f4e-b068607564bd.\n",
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 6.0 failed 1 times, most recent failure: Lost task 1.0 in stage 6.0 (TID 13) (aluno-3886.wireless.ua.pt executor driver): org.apache.spark.SparkException: Task failed while writing rows.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:655)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:358)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$22(FileFormatWriter.scala:266)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "Caused by: java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.util.Arrays.copyOf(Arrays.java:3689)\n",
      "\tat java.base/java.util.ArrayList.grow(ArrayList.java:238)\n",
      "\tat java.base/java.util.ArrayList.addAll(ArrayList.java:710)\n",
      "\tat net.razorvine.pickle.Unpickler.load_appends(Unpickler.java:726)\n",
      "\tat net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:232)\n",
      "\tat net.razorvine.pickle.Unpickler.load(Unpickler.java:109)\n",
      "\tat net.razorvine.pickle.Unpickler.loads(Unpickler.java:122)\n",
      "\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.$anonfun$evaluate$6(BatchEvalPythonExec.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$Lambda$3854/0x00000001015e3840.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:341)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$$Lambda$4204/0x000000010183fc40.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:348)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$22(FileFormatWriter.scala:266)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$$Lambda$4092/0x00000001017e4040.apply(Unknown Source)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2398/0x0000000101119c40.apply(Unknown Source)\n",
      "\t... 5 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2238)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:255)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:793)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "Caused by: org.apache.spark.SparkException: Task failed while writing rows.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:655)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:358)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$22(FileFormatWriter.scala:266)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\t... 1 more\n",
      "Caused by: java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.util.Arrays.copyOf(Arrays.java:3689)\n",
      "\tat java.base/java.util.ArrayList.grow(ArrayList.java:238)\n",
      "\tat java.base/java.util.ArrayList.addAll(ArrayList.java:710)\n",
      "\tat net.razorvine.pickle.Unpickler.load_appends(Unpickler.java:726)\n",
      "\tat net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:232)\n",
      "\tat net.razorvine.pickle.Unpickler.load(Unpickler.java:109)\n",
      "\tat net.razorvine.pickle.Unpickler.loads(Unpickler.java:122)\n",
      "\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.$anonfun$evaluate$6(BatchEvalPythonExec.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$Lambda$3854/0x00000001015e3840.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:341)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$$Lambda$4204/0x000000010183fc40.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:348)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$22(FileFormatWriter.scala:266)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$$Lambda$4092/0x00000001017e4040.apply(Unknown Source)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2398/0x0000000101119c40.apply(Unknown Source)\n",
      "\t... 5 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:>                                                          (0 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/22 14:35:11 WARN PythonUDFRunner: Incomplete task 3.0 in stage 6 (TID 15) interrupted: Attempting to kill Python Worker\n",
      "23/03/22 14:35:11 WARN PythonUDFRunner: Incomplete task 0.0 in stage 6 (TID 12) interrupted: Attempting to kill Python Worker\n",
      "23/03/22 14:35:11 WARN PythonUDFRunner: Incomplete task 2.0 in stage 6 (TID 14) interrupted: Attempting to kill Python Worker\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 35426)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib/python3.8/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib/python3.8/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib/python3.8/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/home/martinho/.local/lib/python3.8/site-packages/pyspark/accumulators.py\", line 281, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/home/martinho/.local/lib/python3.8/site-packages/pyspark/accumulators.py\", line 253, in poll\n",
      "    if func():\n",
      "  File \"/home/martinho/.local/lib/python3.8/site-packages/pyspark/accumulators.py\", line 257, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/home/martinho/.local/lib/python3.8/site-packages/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "fname_candidate_pairs = f'candidate_pairs_{r}_{b}'\n",
    "if not os.path.exists(fname_candidate_pairs):\n",
    "    df_candidate_pairs_fpless.write.mode('overwrite').parquet(path=fname_candidate_pairs, compression='gzip')\n",
    "\n",
    "df_candidate_pairs_fpless = spark.read.parquet(fname_candidate_pairs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get similar articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_articles(tweet_id: str) -> List[str]:\n",
    "    rows = df_candidate_pairs_fpless \\\n",
    "        .filter(F.array_contains('candidate_pair', tweet_id)) \\\n",
    "        .select(F.array_remove('candidate_pair', tweet_id).alias('sole_candidate')) \\\n",
    "        .select(F.col('sole_candidate')[0].alias('similar_article')) \\\n",
    "        .collect()\n",
    "\n",
    "    return [row.similar_article for row in rows]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of false positives/negatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load sample of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_minhash_sample = df_minhash.sample(fraction=0.1)\n",
    "\n",
    "df_candidate_pairs_sample = create_df_candidate_pairs(create_df_bands_lst(create_df_bands(df_minhash_sample)))\n",
    "\n",
    "df_candidate_pairs_fpless_sample = create_df_candidate_pairs_fpless(df_candidate_pairs_sample, df_minhash_sample, similarity_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the false positive percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 29:=================================================>      (15 + 2) / 17]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/21 18:29:04 ERROR Executor: Exception in task 15.0 in stage 29.0 (TID 67)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder.grow(BufferHolder.java:80)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.codegen.UnsafeWriter.grow(UnsafeWriter.java:63)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.codegen.UnsafeWriter.writeUnalignedBytes(UnsafeWriter.java:127)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.codegen.UnsafeWriter.write(UnsafeWriter.java:110)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$11(EvalPythonExec.scala:137)\n",
      "\tat org.apache.spark.sql.execution.python.EvalPythonExec$$Lambda$3518/0x00000001015b6840.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage39.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage41.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage41.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2421/0x0000000101130440.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "23/03/21 18:29:06 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 15.0 in stage 29.0 (TID 67),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder.grow(BufferHolder.java:80)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.codegen.UnsafeWriter.grow(UnsafeWriter.java:63)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.codegen.UnsafeWriter.writeUnalignedBytes(UnsafeWriter.java:127)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.codegen.UnsafeWriter.write(UnsafeWriter.java:110)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$11(EvalPythonExec.scala:137)\n",
      "\tat org.apache.spark.sql.execution.python.EvalPythonExec$$Lambda$3518/0x00000001015b6840.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage39.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage41.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage41.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2421/0x0000000101130440.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "23/03/21 18:29:06 WARN TaskSetManager: Lost task 15.0 in stage 29.0 (TID 67) (192.168.1.66 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder.grow(BufferHolder.java:80)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.codegen.UnsafeWriter.grow(UnsafeWriter.java:63)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.codegen.UnsafeWriter.writeUnalignedBytes(UnsafeWriter.java:127)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.codegen.UnsafeWriter.write(UnsafeWriter.java:110)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$11(EvalPythonExec.scala:137)\n",
      "\tat org.apache.spark.sql.execution.python.EvalPythonExec$$Lambda$3518/0x00000001015b6840.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage39.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage41.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage41.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2421/0x0000000101130440.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "\n",
      "23/03/21 18:29:06 ERROR TaskSetManager: Task 15 in stage 29.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 29:=================================================>      (15 + 1) / 17]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/21 18:29:11 ERROR Executor: Exception in task 16.0 in stage 29.0 (TID 68): Java heap space\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 50852)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib/python3.8/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib/python3.8/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib/python3.8/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/home/martinho/.local/lib/python3.8/site-packages/pyspark/accumulators.py\", line 281, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/home/martinho/.local/lib/python3.8/site-packages/pyspark/accumulators.py\", line 253, in poll\n",
      "    if func():\n",
      "  File \"/home/martinho/.local/lib/python3.8/site-packages/pyspark/accumulators.py\", line 257, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/home/martinho/.local/lib/python3.8/site-packages/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/martinho/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3398, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_42510/2566506517.py\", line 1, in <cell line: 1>\n",
      "    print('Percentage of false positives:', (df_candidate_pairs_sample.count() - df_candidate_pairs_fpless_sample.count()) / df_minhash_sample.count())\n",
      "  File \"/home/martinho/.local/lib/python3.8/site-packages/pyspark/sql/dataframe.py\", line 804, in count\n",
      "    return int(self._jdf.count())\n",
      "  File \"/home/martinho/.local/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/home/martinho/.local/lib/python3.8/site-packages/pyspark/sql/utils.py\", line 190, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/home/martinho/.local/lib/python3.8/site-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: <unprintable Py4JJavaError object>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/martinho/.local/lib/python3.8/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/martinho/.local/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/martinho/.local/lib/python3.8/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 111] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[1;32m/home/martinho/ua/4-2/mdle/assign1/exercise2.ipynb Cell 47\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/martinho/ua/4-2/mdle/assign1/exercise2.ipynb#X62sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mPercentage of false positives:\u001b[39m\u001b[39m'\u001b[39m, (df_candidate_pairs_sample\u001b[39m.\u001b[39;49mcount() \u001b[39m-\u001b[39m df_candidate_pairs_fpless_sample\u001b[39m.\u001b[39mcount()) \u001b[39m/\u001b[39m df_minhash_sample\u001b[39m.\u001b[39mcount())\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/dataframe.py:804\u001b[0m, in \u001b[0;36mDataFrame.count\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[39m\"\"\"Returns the number of rows in this :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m    796\u001b[0m \n\u001b[1;32m    797\u001b[0m \u001b[39m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    802\u001b[0m \u001b[39m2\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 804\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mint\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jdf\u001b[39m.\u001b[39;49mcount())\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m     \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    191\u001b[0m \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31m<class 'str'>\u001b[0m: (<class 'ConnectionRefusedError'>, ConnectionRefusedError(111, 'Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py:2004\u001b[0m, in \u001b[0;36mInteractiveShell.showtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2001\u001b[0m     traceback\u001b[39m.\u001b[39mprint_exc()\n\u001b[1;32m   2002\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 2004\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_showtraceback(etype, value, stb)\n\u001b[1;32m   2005\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcall_pdb:\n\u001b[1;32m   2006\u001b[0m     \u001b[39m# drop into debugger\u001b[39;00m\n\u001b[1;32m   2007\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdebugger(force\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/ipykernel/zmqshell.py:538\u001b[0m, in \u001b[0;36mZMQInteractiveShell._showtraceback\u001b[0;34m(self, etype, evalue, stb)\u001b[0m\n\u001b[1;32m    532\u001b[0m sys\u001b[39m.\u001b[39mstdout\u001b[39m.\u001b[39mflush()\n\u001b[1;32m    533\u001b[0m sys\u001b[39m.\u001b[39mstderr\u001b[39m.\u001b[39mflush()\n\u001b[1;32m    535\u001b[0m exc_content \u001b[39m=\u001b[39m {\n\u001b[1;32m    536\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtraceback\u001b[39m\u001b[39m\"\u001b[39m: stb,\n\u001b[1;32m    537\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mename\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mstr\u001b[39m(etype\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m),\n\u001b[0;32m--> 538\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mevalue\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mstr\u001b[39;49m(evalue),\n\u001b[1;32m    539\u001b[0m }\n\u001b[1;32m    541\u001b[0m dh \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdisplayhook\n\u001b[1;32m    542\u001b[0m \u001b[39m# Send exception info over pub socket for other clients than the caller\u001b[39;00m\n\u001b[1;32m    543\u001b[0m \u001b[39m# to pick up\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/protocol.py:471\u001b[0m, in \u001b[0;36mPy4JJavaError.__str__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__str__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    470\u001b[0m     gateway_client \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjava_exception\u001b[39m.\u001b[39m_gateway_client\n\u001b[0;32m--> 471\u001b[0m     answer \u001b[39m=\u001b[39m gateway_client\u001b[39m.\u001b[39;49msend_command(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexception_cmd)\n\u001b[1;32m    472\u001b[0m     return_value \u001b[39m=\u001b[39m get_return_value(answer, gateway_client, \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    473\u001b[0m     \u001b[39m# Note: technically this should return a bytestring 'str' rather than\u001b[39;00m\n\u001b[1;32m    474\u001b[0m     \u001b[39m# unicodes in Python 2; however, it can return unicodes for now.\u001b[39;00m\n\u001b[1;32m    475\u001b[0m     \u001b[39m# See https://github.com/bartdag/py4j/issues/306 for more details.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msend_command\u001b[39m(\u001b[39mself\u001b[39m, command, retry\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, binary\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m   1016\u001b[0m     \u001b[39m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[39m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[39m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[39m     if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1036\u001b[0m     connection \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_connection()\n\u001b[1;32m   1037\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m         response \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[39mif\u001b[39;00m connection \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m connection\u001b[39m.\u001b[39msocket \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     connection \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_new_connection()\n\u001b[1;32m    285\u001b[0m \u001b[39mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_create_new_connection\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    288\u001b[0m     connection \u001b[39m=\u001b[39m ClientServerConnection(\n\u001b[1;32m    289\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjava_parameters, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpython_parameters,\n\u001b[1;32m    290\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_property, \u001b[39mself\u001b[39m)\n\u001b[0;32m--> 291\u001b[0m     connection\u001b[39m.\u001b[39;49mconnect_to_java_server()\n\u001b[1;32m    292\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_thread_connection(connection)\n\u001b[1;32m    293\u001b[0m     \u001b[39mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mssl_context:\n\u001b[1;32m    436\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msocket \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mssl_context\u001b[39m.\u001b[39mwrap_socket(\n\u001b[1;32m    437\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msocket, server_hostname\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjava_address)\n\u001b[0;32m--> 438\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msocket\u001b[39m.\u001b[39;49mconnect((\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mjava_address, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mjava_port))\n\u001b[1;32m    439\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msocket\u001b[39m.\u001b[39mmakefile(\u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    440\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_connected \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused"
     ]
    }
   ],
   "source": [
    "print('Percentage of false positives:', (df_candidate_pairs_sample.count() - df_candidate_pairs_fpless_sample.count()) / df_minhash_sample.count())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the false negative percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_minhash_sample.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "18897"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_shingles.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "171"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_candidate_pairs_sample.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (1621472720.py, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[151], line 7\u001b[0;36m\u001b[0m\n\u001b[0;31m    .count()\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "df_minhash_sample \\\n",
    "    .crossJoin(df_minhash_sample.select(F.col('tweet_id').alias('tweet_id_other'), F.col('min_hash').alias('min_hash_other'))) \\\n",
    "    .filter(F.col('tweet_id') < F.col('tweet_id_other')) \\\n",
    "    .select(F.array('tweet_id', 'tweet_id_other').alias('pair'),'min_hash', 'min_hash_other') \\\n",
    "    .join(df_candidate_pairs_sample, df_candidate_pairs_sample['candidate_pair'] == F.col('pair'), 'left') \\\n",
    "    .filter(F.col('candidate_pair').isNull()) \\\n",
    "    .count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[tweet_id: string, min_hash: array<int>]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_minhash"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
