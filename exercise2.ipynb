{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise the LSH algorithm was developed to identify similar news articles. This algorithm was implemented using Spark, more specifically the PySpark library with the Dataframe API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PySpark is the only non-standard library required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import random\n",
    "import math\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession, Row, DataFrame\n",
    "from pyspark.sql.types import StringType, ArrayType, IntegerType, StructType, StructField, DecimalType, DoubleType\n",
    "from itertools import combinations, chain\n",
    "from functools import partial\n",
    "from typing import Iterable, Any, List, Callable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values for parameters $b$ and $r$ chosen, according to the requirements, were:\n",
    "- $b = 13$\n",
    "- $r = 11$\n",
    "\n",
    "The values were hand-picked by visually analyzing the plot for the probability of two documents sharing a bucket depending on their similarity, as $b$ and $r$ changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not plot, since the 'matplotlib' module is not present.\n"
     ]
    }
   ],
   "source": [
    "N = 100\n",
    "\n",
    "r = 11\n",
    "b = 13\n",
    "\n",
    "point_below = (0.85, 0.9)\n",
    "point_above = (0.6, 0.05)\n",
    "\n",
    "prob = lambda s, r, b: 1 - (1 - s**r)**b\n",
    "\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    ss = [i/N for i in range(N)]\n",
    "\n",
    "    plt.plot(*point_below, color='g', marker='o')\n",
    "    plt.plot(*point_above, color='r', marker='o')\n",
    "    plt.plot(ss, [prob(s, r, b) for s in ss])\n",
    "\n",
    "    plt.title(f'Probability of two documents sharing a bucket w.r.t. their similarity $s$\\n($r={r}$, $b={b}$)')\n",
    "    plt.legend(['at least', 'less than', 'probability'])\n",
    "    plt.xlabel('$s$')\n",
    "    plt.ylabel('probability')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "except ImportError:\n",
    "    print('Could not plot, since the \\'matplotlib\\' module is not present.')\n",
    "\n",
    "assert prob(point_below[0], r, b) >= point_below[1], 'Pairs with a similarity of 85%% should have at least 90%% probability of sharing a bucket!'\n",
    "assert prob(point_above[0], r, b) <  point_above[1], 'Pairs with a similarity of 60%% should have less than 5%% probability of sharing a bucket!'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have defined all the variables used as parameters for the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shingle size\n",
    "k = 9\n",
    "\n",
    "# Number of bands\n",
    "b = 13\n",
    "\n",
    "# Number of rows per band\n",
    "r = 11\n",
    "\n",
    "# Min-hash: number of hash functions\n",
    "num_functions = b*r\n",
    "\n",
    "# Seed for the random number generator\n",
    "seed = 123\n",
    "\n",
    "# TODO: Similarity threshold, what is this?\n",
    "similarity_threshold = 0.85\n",
    "\n",
    "# Sample of the dataset to use\n",
    "sample_fraction = 0.01\n",
    "\n",
    "# Number of explicit partitions\n",
    "num_partitions = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(seed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Initialization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark is initialized, with as many worker threads as logical cores on the machine.\n",
    "We did not use a fixed value since the machines used for development had a different number of CPU cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName('LSH') \\\n",
    "    .config('spark.master', 'local[*]') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is about news in Twitter, where each row identifies a tweet id and it's text as well as the URL for the tweet.\n",
    "\n",
    "The data's format is json, and is loaded to a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# TODO: Configure partitions for speedup?\n",
    "df = spark.read \\\n",
    "    .json('./data/covid_news_small.json.bz2') \\\n",
    "    .repartition(num_partitions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframe `df` will have three columns: `text`, `tweet_id` and `url`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate shingles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: ignore punctuation? use different shingling strategy? (see 'Further fun' slide, which is the last, of 3b)\n",
    "@F.udf(returnType=ArrayType(IntegerType(), False))\n",
    "def generate_shingles(text: str):\n",
    "    shingles = (text[idx:idx+k] for idx in range(len(text) - k + 1))\n",
    "    # Get last 32 bits in order to have 4-byte integers (Python allows arbitrarily large integers)\n",
    "    to_integer = lambda s: hash(s) & ((1 << 32) - 1)\n",
    "    return list(set(to_integer(shingle_str) for shingle_str in shingles))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step for the algorithm is to generate the shingles for each document/tweet.\n",
    "We acomplish this by removing all the tweets which won't have at least on shingle of size `k` using a filter.\n",
    "Then we use a UDF to create all the shingles of each `text` and immediately hashing them to a 32 bit number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shingles = df \\\n",
    "    .drop('url') \\\n",
    "    .filter(F.length('text') >= k) \\\n",
    "    .repartition(num_partitions) \\\n",
    "    .withColumn('shingles', generate_shingles('text')) \\\n",
    "    .drop('text')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this, the dataframe `df_shingles` will be composed of two columns: `tweet_id` and `shingles`, the latter being an array of the hashed shingles for this tweet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Min-hash"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to generate the min-hash signatures.\n",
    "\n",
    "First we need to generate the hash functions. We will use the following function to generate `num_functions` hash functions, using the universal hash function contained in the lecture slides.\n",
    "Our `N` is the number of possible shingles (in this case our hashed shingles are 32-bit integers, so `N` is `2**32`), and `p` is a prime number larger than `N`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assumes the values to hash are 4-byte integers\n",
    "def generate_universal_hash_family(K: int) -> List[Callable[[int], int]]:\n",
    "    N = 1 << 32\n",
    "    p = 2305843009213693951\n",
    "\n",
    "    parameters = set()\n",
    "    while (len(parameters) < K):\n",
    "        parameters |= {(random.randint(1, N), random.randint(0, N)) for _ in range(K - len(parameters))}\n",
    "    \n",
    "    return [(a, b, p, N) for a, b in parameters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_family = generate_universal_hash_family(num_functions)\n",
    "broadcasted_hash_family = spark.sparkContext.broadcast(hash_family)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then use just need to use the generated hash functions to calculate the min-hash signatures for each tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "@F.udf(returnType=ArrayType(IntegerType(), False))\n",
    "def calculate_min_hash(shingles: List[int]):\n",
    "    return [min(((a * shingle + b) % p) % N for shingle in shingles) for (a, b, p, N) in broadcasted_hash_family.value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_minhash = df_shingles.withColumn('min_hash', calculate_min_hash('shingles')).drop('shingles')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this, the dataframe `df_minhash` will be composed of two columns: `tweet_id` and `min_hash`, where `tweet_id` is the id of the document/tweet and `min_hash` is a list of integers, each one being the result of applying one of the hash functions to the shingles of the document/tweet calculated using the calculate_min_hash UDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#t = time.time()\n",
    "#hash_col_names = [f'hashed_{i}' for i in range(num_functions)]\n",
    "\n",
    "#data = df_shingles \\\n",
    "#    .withColumn('shingles', F.explode('shingles')) \\\n",
    "#    .withColumnRenamed('shingles', 'shingle') \\\n",
    "#    .select('tweet_id', *( (((a * F.col('shingle').cast(DecimalType()) + b) % p) % N).alias(name) for (a, b, p, N), name in zip(hash_family, hash_col_names) )) \\\n",
    "#    .groupby('tweet_id') \\\n",
    "#    .min(*hash_col_names) \\\n",
    "#    .withColumn('min_hash', F.array(*(f'min({name})' for name in hash_col_names))) \\\n",
    "#    .select('tweet_id', 'min_hash') \\\n",
    "#    .collect()\n",
    "\n",
    "#print('Execution time:', time.time() - t)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The min_hash results are saved in disk in Parquet format (Spark's default format) for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname_minhash = f'minhash_{r}_{b}'\n",
    "if not os.path.exists(fname_minhash):\n",
    "    df_minhash.write.mode('overwrite').parquet(path=fname_minhash, compression='gzip')\n",
    "\n",
    "df_minhash = spark.read.parquet(fname_minhash)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSH"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step is to aplly the LSH algorithm to the minhashes. We will use the LSH algorithm described in the slides.\n",
    "\n",
    "First we need to divide the min_hash signatures into `b` bands, each of size `r`. We will used the following UDF to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "@F.udf(returnType=ArrayType(ArrayType(IntegerType(), False), False))\n",
    "def generate_even_slices(minhashes: List[int]):\n",
    "    return [minhashes[i:i+r] for i in range(0, num_functions, r)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, we need to hash all the min_hash values of each text of each band to obtain the bucket identifiers of each band.\n",
    "For this we use the hash function of the spark library, creating a column named bands which will have the bucket identifier (`band_hash`) and the `band` number for each `tweet_id`.\n",
    "\n",
    "At the end of the function we separate the `band_hash` and `band` columns into two different columns, one for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_bands(df: DataFrame) -> DataFrame:\n",
    "    return df \\\n",
    "        .withColumn('min_hash_slices', generate_even_slices('min_hash')) \\\n",
    "        .withColumn('bands', F.array(*(\n",
    "            F.struct(\n",
    "                F.hash(F.col('min_hash_slices')[band]).alias('band_hash'),\n",
    "                F.lit(band).alias('band')\n",
    "            )\n",
    "            for band in range(b))\n",
    "        )) \\\n",
    "        .withColumn('bands', F.explode('bands')) \\\n",
    "        .select('tweet_id', F.col('bands').band.alias('band'), F.col('bands').band_hash.alias('band_hash'))\n",
    "\n",
    "df_bands = create_df_bands(df_minhash)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This leaves us with the dataframe `df_bands`, which will be composed of three columns: `tweet_id`, `band` and `band_hash`, the latter being the bucket identifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "@F.udf(returnType=ArrayType(ArrayType(StringType(), False), False))\n",
    "def combine_pairs(elems: Iterable[Any]):\n",
    "    return list(combinations(elems, 2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having the buckets for each document/tweet in a band, we can now generate the pairs of documents/tweets that are candidates for being similar.\n",
    "For this, we begin by grouping the documents/tweets by `band` and `band_hash`.\n",
    "This, with a `collect_list`, gives us a list of tweets for each bucket, called `candidates`.\n",
    "Then we sort the `candidates` list to facilitate the removal of duplicate pairs, and filter the rows that have only one tweet ot less.\n",
    "By doing a select of `candidates` we can remove most of the duplicates by a distinct.\n",
    "Finally, we explode the `candidates` column to get the pairs of tweets and separate them into two columns, named `candidate_pair_first` and `candidate_pair_second`.\n",
    "To remove the duplicates generated by the combinations, we filter them using distinct()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_candidate_pairs(df: DataFrame) -> DataFrame:\n",
    "    return df \\\n",
    "        .groupby('band', 'band_hash') \\\n",
    "        .agg(F.collect_list('tweet_id')) \\\n",
    "        .withColumnRenamed('collect_list(tweet_id)', 'candidates') \\\n",
    "        .withColumn('candidates', F.array_sort('candidates')) \\\n",
    "        .filter(F.size('candidates') > 1) \\\n",
    "        .repartition(num_partitions) \\\n",
    "        .select('candidates') \\\n",
    "        .distinct() \\\n",
    "        .select(F.explode(combine_pairs('candidates')).alias('candidate_pair')) \\\n",
    "        .select(F.col('candidate_pair')[0].alias('candidate_pair_first'), F.col('candidate_pair')[1].alias('candidate_pair_second')) \\\n",
    "        .distinct() \n",
    "\n",
    "df_candidate_pairs = create_df_candidate_pairs(df_bands)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, before saving the results on disk, we remove the false positives.\n",
    "To verify if a given pair is a false positive, we compare the min_hash values of each document of the pair using the UDF below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "@F.udf(returnType=IntegerType())\n",
    "def min_hash_similar(min_hash_0: List[int], min_hash_1: List[int]):\n",
    "    return sum((elem0 == elem1) for elem0, elem1 in zip(min_hash_0, min_hash_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_candidate_pairs_fpless(df_candidate_pairs: DataFrame, df_minhash: DataFrame, similarity_threshold: float) -> DataFrame:\n",
    "    return df_candidate_pairs \\\n",
    "        .join(df_minhash, df_minhash['tweet_id'] == F.col('candidate_pair_first')) \\\n",
    "        .withColumnRenamed('min_hash', 'min_hash_first') \\\n",
    "        .drop('tweet_id') \\\n",
    "        .join(df_minhash, df_minhash['tweet_id'] == F.col('candidate_pair_second')) \\\n",
    "        .withColumnRenamed('min_hash', 'min_hash_second') \\\n",
    "        .drop('tweet_id') \\\n",
    "        .withColumn('similarity', min_hash_similar('min_hash_first', 'min_hash_second') / num_functions) \\\n",
    "        .filter(F.col('similarity') >= similarity_threshold)\n",
    "\n",
    "df_candidate_pairs_fpless = create_df_candidate_pairs_fpless(df_candidate_pairs, df_minhash, similarity_threshold)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This leaves us with the dataframe `df_candidate_pairs_fpless`, which will be composed of five columns: `candidate_pair_first`, `candidate_pair_second`, `min_hash_first`,`min_hash_second` and `similarity`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save results into disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname_candidate_pairs = f'candidate_pairs_{r}_{b}'\n",
    "if not os.path.exists(fname_candidate_pairs):\n",
    "    df_candidate_pairs_fpless.write.mode('overwrite').parquet(path=fname_candidate_pairs, compression='gzip')\n",
    "\n",
    "df_candidate_pairs_fpless = spark.read.parquet(fname_candidate_pairs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For exercise 2.2 we developed a function to get similar articles.\n",
    "\n",
    "In this function we filter all the pairs which have the given document/tweet_id and create an array with all the similar articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_articles(tweet_id: str) -> List[str]:\n",
    "    rows = df_candidate_pairs_fpless \\\n",
    "        .withColumn('candidate_pair_first', F.when(F.col('candidate_pair_first') != tweet_id, F.col('candidate_pair_first'))) \\\n",
    "        .withColumn('candidate_pair_second', F.when(F.col('candidate_pair_second') != tweet_id, F.col('candidate_pair_second'))) \\\n",
    "        .filter(F.col('candidate_pair_first').isNull() | F.col('candidate_pair_second').isNull()) \\\n",
    "        .select(F.coalesce('candidate_pair_first', 'candidate_pair_second').alias('similar_article')) \\\n",
    "        .collect()\n",
    "\n",
    "    return [row.similar_article for row in rows]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of false positives/negatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we load sample of the min_hash data to do the analysis of false positives and negatives.\n",
    "\n",
    "Then we generate the candidate pairs like before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_minhash_sample = df_minhash.sample(fraction=0.1, seed=seed, withReplacement=False)\n",
    "\n",
    "df_candidate_pairs_sample = create_df_candidate_pairs(create_df_bands(df_minhash_sample))\n",
    "\n",
    "df_candidate_pairs_fpless_sample = create_df_candidate_pairs_fpless(df_candidate_pairs_sample, df_minhash_sample, similarity_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6997"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_candidate_pairs_sample.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5645"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_candidate_pairs_fpless_sample.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the false positive percentage (false discovery rate).\n",
    "\n",
    "Since we have the dataframe of candidate pairs and the dataframe of candidate pairs without false positives, we can get the number of false positives by subtracting the number of rows of the dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of false positives: 19.322567%\n"
     ]
    }
   ],
   "source": [
    "# TODO: is this what we are supposed to calculate?\n",
    "print(f'Percentage of false positives: {(df_candidate_pairs_sample.count() - df_candidate_pairs_fpless_sample.count()) / df_candidate_pairs_sample.count():%}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the false negative percentage (false omission rate).\n",
    "The following code gets us the number of false negatives by comparing the values of the sample of min_hashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|                pair|            min_hash|      min_hash_other|          similarity|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|[1375958178026901...|[4173847, 522047,...|[1867425, 3935602...|0.027972027972027972|\n",
      "|[1375958178026901...|[4173847, 522047,...|[13250592, 486195...|0.013986013986013986|\n",
      "|[1375958178026901...|[4173847, 522047,...|[181913, 643007, ...|0.006993006993006993|\n",
      "|[1375958178026901...|[4173847, 522047,...|[13250592, 486195...|0.013986013986013986|\n",
      "|[1375958178026901...|[4173847, 522047,...|[13250592, 486195...|0.013986013986013986|\n",
      "|[1375958178026901...|[4173847, 522047,...|[822262, 731778, ...|0.013986013986013986|\n",
      "|[1375958178026901...|[4173847, 522047,...|[1867425, 1306842...|0.013986013986013986|\n",
      "|[1375958178026901...|[4173847, 522047,...|[857035, 2891616,...|0.006993006993006993|\n",
      "|[1375958178026901...|[4173847, 522047,...|[1867425, 1329833...|0.006993006993006993|\n",
      "|[1375958178026901...|[4173847, 522047,...|[768318, 2504859,...|                 0.0|\n",
      "|[1375958178026901...|[4173847, 522047,...|[611093, 3935602,...|0.006993006993006993|\n",
      "|[1375958178026901...|[4173847, 522047,...|[6391927, 486195,...|0.013986013986013986|\n",
      "|[1375958178026901...|[4173847, 522047,...|[964161, 1930820,...| 0.02097902097902098|\n",
      "|[1375958178026901...|[4173847, 522047,...|[168230074, 22583...|                 0.0|\n",
      "|[1375958178026901...|[4173847, 522047,...|[1867425, 1508433...| 0.02097902097902098|\n",
      "|[1375958178026901...|[4173847, 522047,...|[3629636, 486195,...|0.006993006993006993|\n",
      "|[1375958178026901...|[4173847, 522047,...|[5507662, 1410680...|0.006993006993006993|\n",
      "|[1375958178026901...|[4173847, 522047,...|[1348263, 1966385...|                 0.0|\n",
      "|[1375958178026901...|[4173847, 522047,...|[454720, 1377166,...|                 0.0|\n",
      "|[1375958178026901...|[4173847, 522047,...|[6309098, 486195,...|0.013986013986013986|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_minhash_sample \\\n",
    "    .crossJoin(df_minhash_sample.select(F.col('tweet_id').alias('tweet_id_other'), F.col('min_hash').alias('min_hash_other'))) \\\n",
    "    .filter(F.col('tweet_id') < F.col('tweet_id_other')) \\\n",
    "    .select(F.array('tweet_id', 'tweet_id_other').alias('pair'),'min_hash', 'min_hash_other') \\\n",
    "    .join(df_candidate_pairs_sample, F.array(df_candidate_pairs_sample['candidate_pair_first'], df_candidate_pairs_sample['candidate_pair_second']) == F.col('pair'), 'left') \\\n",
    "    .filter(F.col('candidate_pair_first').isNull()) \\\n",
    "    .drop('candidate_pair_first', 'candidate_pair_second') \\\n",
    "    .withColumn('similarity', min_hash_similar('min_hash', 'min_hash_other') / num_functions) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "false_negatives = df_minhash_sample \\\n",
    "    .crossJoin(df_minhash_sample.select(F.col('tweet_id').alias('tweet_id_other'), F.col('min_hash').alias('min_hash_other'))) \\\n",
    "    .filter(F.col('tweet_id') < F.col('tweet_id_other')) \\\n",
    "    .select(F.array('tweet_id', 'tweet_id_other').alias('pair'),'min_hash', 'min_hash_other') \\\n",
    "    .join(df_candidate_pairs_sample, F.array(df_candidate_pairs_sample['candidate_pair_first'], df_candidate_pairs_sample['candidate_pair_second']) == F.col('pair'), 'left') \\\n",
    "    .filter(F.col('candidate_pair_first').isNull()) \\\n",
    "    .drop('candidate_pair_first', 'candidate_pair_second') \\\n",
    "    .withColumn('similarity', min_hash_similar('min_hash', 'min_hash_other') / num_functions) \\\n",
    "    .filter(F.col('similarity') >= similarity_threshold) \\\n",
    "    .count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the percentage of false negatives we used the count previously calculated and divide it by the total number of negatives detected, which is obtained by subtracting the number of candidate pairs to the number of combinations possible of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 311:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of false negatives: 0.000000%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(f'Percentage of false negatives: {false_negatives / (math.comb(df_minhash_sample.count(), 2) - df_candidate_pairs.count()):%}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
