{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise the LSH algorithm was developed to identify similar news articles. This algorithm was implemented using Spark, more specifically the PySpark library with the Dataframe API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PySpark is the only non-standard library required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import random\n",
    "import math\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.types import StringType, ArrayType, IntegerType\n",
    "from itertools import combinations\n",
    "from typing import Iterable, Any, List, Callable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values for parameters $b$ and $r$ chosen, according to the requirements (2.1), were:\n",
    "- $b = 13$\n",
    "- $r = 11$\n",
    "\n",
    "The values were hand-picked by visually analyzing the plot for the probability of two documents sharing a bucket depending on their similarity, as $b$ and $r$ changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not plot, since the 'matplotlib' module is not present.\n"
     ]
    }
   ],
   "source": [
    "N = 100\n",
    "\n",
    "r = 11\n",
    "b = 13\n",
    "\n",
    "point_below = (0.85, 0.9)\n",
    "point_above = (0.6, 0.05)\n",
    "\n",
    "prob = lambda s, r, b: 1 - (1 - s**r)**b\n",
    "\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    ss = [i/N for i in range(N)]\n",
    "\n",
    "    plt.plot(*point_below, color='g', marker='o')\n",
    "    plt.plot(*point_above, color='r', marker='o')\n",
    "    plt.plot(ss, [prob(s, r, b) for s in ss])\n",
    "\n",
    "    plt.title(f'Probability of two documents sharing a bucket w.r.t. their similarity $s$\\n($r={r}$, $b={b}$)')\n",
    "    plt.legend(['at least', 'less than', 'probability'])\n",
    "    plt.xlabel('$s$')\n",
    "    plt.ylabel('probability')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "except ImportError:\n",
    "    print('Could not plot, since the \\'matplotlib\\' module is not present.')\n",
    "\n",
    "assert prob(point_below[0], r, b) >= point_below[1], 'Pairs with a similarity of 85%% should have at least 90%% probability of sharing a bucket!'\n",
    "assert prob(point_above[0], r, b) <  point_above[1], 'Pairs with a similarity of 60%% should have less than 5%% probability of sharing a bucket!'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have defined all the algorithm's paramters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shingle size\n",
    "k = 9\n",
    "\n",
    "# Number of bands\n",
    "b = 13\n",
    "\n",
    "# Number of rows per band\n",
    "r = 11\n",
    "\n",
    "# Min-hash: number of hash functions\n",
    "num_functions = b*r\n",
    "\n",
    "# Seed for the random number generator\n",
    "seed = 1\n",
    "\n",
    "# Similarity threshold\n",
    "similarity_threshold = 0.85\n",
    "\n",
    "# Number of explicit partitions\n",
    "num_partitions = 8"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The seed for the random number generator is set, which will be used when generating the min-hash hash function family and obtaining a sample of the dataset for false positive/negative analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(seed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Initialization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark is initialized, with as many worker threads as logical cores on the machine.\n",
    "We did not use a fixed value since the machines used for development had a different number of CPU cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName('LSH') \\\n",
    "    .config('spark.master', 'local[*]') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is about news in Twitter, where each row identifies a tweet ID, URL and text.\n",
    "\n",
    "The data's format is JSON, and is loaded to a dataframe.\n",
    "The data is partitioned using a fixed number of partitions, and it will be repartitioned again in the future.\n",
    "This helps alleviate the impact that `filter` operations have on the partitions, which were heavily hampering the algorithm's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read \\\n",
    "    .json('./data/covid_news_small.json.bz2') \\\n",
    "    .repartition(num_partitions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframe `df` will have three columns: `text`, `tweet_id` and `url`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate shingles"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step for the algorithm is to generate the shingles for each document/tweet.\n",
    "We acomplish this by removing all the tweets which won't have at least one shingle of size `k` using a filter.\n",
    "The data is partitioned after filtering to avoid data skew.\n",
    "\n",
    "Then we use a UDF to create all the shingles of each `text`.\n",
    "Within the UDF, each shingle which will also be hashed to a 32 bit integer, so that it can be stored in a Spark `IntegerType`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "@F.udf(returnType=ArrayType(IntegerType(), False))\n",
    "def generate_shingles(text: str):\n",
    "    shingles = (text[idx:idx+k] for idx in range(len(text) - k + 1))\n",
    "    # Get last 32 bits in order to have 4-byte integers (Python allows arbitrarily large integers)\n",
    "    to_integer = lambda s: hash(s) & ((1 << 32) - 1)\n",
    "    return list(set(map(to_integer, shingles)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shingles = df \\\n",
    "    .drop('url') \\\n",
    "    .filter(F.length('text') >= k) \\\n",
    "    .repartition(num_partitions) \\\n",
    "    .withColumn('shingles', generate_shingles('text')) \\\n",
    "    .drop('text')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this, the dataframe `df_shingles` will be composed of two columns: `tweet_id` and `shingles`, the latter being an array of the hashed shingles for this tweet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Min-hash"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to generate the min-hash signatures.\n",
    "\n",
    "First we need to generate the hash functions. We will use the following function to generate `num_functions` hash functions from a universal hash family of the form\n",
    "$$\n",
    "((a \\times x + b) \\operatorname{mod} p) \\operatorname{mod} N\n",
    "$$.\n",
    "Our `N` is the number of possible shingles (in this case our hashed shingles are 32-bit integers, so `N` is $2^{32}$), and `p` is a prime number larger than `N`.\n",
    "\n",
    "We randomly generate unique pairs of `a` and `b`, and the hash functions will be represented by all the four parameters (even though `p` and `N` end up being the same)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assumes the values to hash are 4-byte integers\n",
    "def generate_universal_hash_family(K: int) -> List[Callable[[int], int]]:\n",
    "    N = 1 << 32\n",
    "    p = 2305843009213693951\n",
    "\n",
    "    parameters = set()\n",
    "    while (len(parameters) < K):\n",
    "        parameters |= {(random.randint(1, N), random.randint(0, N)) for _ in range(K - len(parameters))}\n",
    "    \n",
    "    return [(a, b, p, N) for a, b in parameters]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We explicitly broadcast the generated hash functions to all nodes, so that the hash parameters can be easily accessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_family = generate_universal_hash_family(num_functions)\n",
    "broadcasted_hash_family = spark.sparkContext.broadcast(hash_family)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we just need to use the generated hash functions to calculate the min-hash signatures for each tweet.\n",
    "An UDF was used, especially since Python allows arbitrarily large integers, and the numbers involved in calculating the hash values are very large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "@F.udf(returnType=ArrayType(IntegerType(), False))\n",
    "def calculate_min_hash(shingles: List[int]):\n",
    "    return [min(((a * shingle + b) % p) % N for shingle in shingles) for (a, b, p, N) in broadcasted_hash_family.value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_minhash(df_shingles: DataFrame) -> DataFrame:\n",
    "    return df_shingles.withColumn('min_hash', calculate_min_hash('shingles')).drop('shingles')\n",
    "\n",
    "df_minhash = create_df_minhash(df_shingles)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this, the dataframe `df_minhash` will be composed of two columns: `tweet_id` and `min_hash`, where `tweet_id` is the ID of the document/tweet and `min_hash` is a list of integers, each one being the result of applying one of the hash functions to the shingles of the document/tweet calculated using the `calculate_min_hash` UDF."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The min-hash results are saved in disk in Parquet format (Spark's default format) for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname_minhash = f'minhash_{r}_{b}'\n",
    "if not os.path.exists(fname_minhash):\n",
    "    df_minhash.write.mode('overwrite').parquet(path=fname_minhash, compression='gzip')\n",
    "\n",
    "df_minhash = spark.read.parquet(fname_minhash)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSH"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step is to apply the LSH algorithm to the min-hashes and obtain the candidate pairs.\n",
    "\n",
    "First we need to divide the min-hash signatures into `b` bands, each of size `r`. We developed the following UDF to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "@F.udf(returnType=ArrayType(ArrayType(IntegerType(), False), False))\n",
    "def generate_even_slices(minhashes: List[int]):\n",
    "    return [minhashes[i:i+r] for i in range(0, num_functions, r)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After dividing the min-hash signatures in even slices with the previous UDF, we need to hash all the min-hash values of each band to obtain the band-specific bucket identifiers.\n",
    "For this we use the hash function of the Spark library, creating a column named `bands` which will have an array of pairs, with 2 items:\n",
    "- bucket identifier (`band_hash`), which is the hash value applied to a given band;\n",
    "- the `band` number/identifier to which the bucket is associated to (integer in $[0, b)$).\n",
    "\n",
    "This array is pairs is exploded into rows, one for each pair, which will allow grouping operations in the future.\n",
    "\n",
    "At the end of the function we separate the `band_hash` and `band` columns into two different columns, one for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_bands(df_minhash: DataFrame) -> DataFrame:\n",
    "    return df_minhash \\\n",
    "        .withColumn('min_hash_slices', generate_even_slices('min_hash')) \\\n",
    "        .withColumn('bands', F.array(*(\n",
    "            F.struct(\n",
    "                F.hash(F.col('min_hash_slices')[band]).alias('band_hash'),\n",
    "                F.lit(band).alias('band')\n",
    "            )\n",
    "            for band in range(b))\n",
    "        )) \\\n",
    "        .withColumn('bands', F.explode('bands')) \\\n",
    "        .select('tweet_id', F.col('bands').band.alias('band'), F.col('bands').band_hash.alias('band_hash'))\n",
    "\n",
    "df_bands = create_df_bands(df_minhash)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This leaves us with the dataframe `df_bands`, which will be composed of three columns: `tweet_id`, `band` and `band_hash`, the latter being the bucket identifier.\n",
    "\n",
    "For the next step, we generate the candidate pairs, and so we reuse the pair-generating UDF from exercise 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "@F.udf(returnType=ArrayType(ArrayType(StringType(), False), False))\n",
    "def combine_pairs(elems: Iterable[Any]):\n",
    "    return list(combinations(elems, 2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having the buckets for each document/tweet in a band, we can now generate the pairs of documents/tweets that are candidates for being similar.\n",
    "\n",
    "For this, we begin by grouping the documents/tweets by `band` and `band_hash` (that is, group the documents that are in the same bucket for a given band).\n",
    "Then, the tweets are aggregated into an array column, called `candidates`.\n",
    "\n",
    "Then we sort the `candidates` arrays to facilitate the removal of duplicate pairs, and filter the rows that have only one tweet ot less.\n",
    "\n",
    "By doing selecting the distinct `candidates` we can remove plenty of buckets that report the same tweets, avoiding possible memory issues when generating pairs (subject to combinatorial explosion).\n",
    "Finally, after generating the pairs within each bucket, we explode the `candidates` column to get the pairs of tweets and separate them into two columns, named `candidate_pair_first` and `candidate_pair_second`.\n",
    "We additionally remove the duplicates generated by the combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_candidate_pairs(df_bands: DataFrame) -> DataFrame:\n",
    "    return df_bands \\\n",
    "        .groupby('band', 'band_hash') \\\n",
    "        .agg(F.collect_list('tweet_id')) \\\n",
    "        .withColumnRenamed('collect_list(tweet_id)', 'candidates') \\\n",
    "        .withColumn('candidates', F.array_sort('candidates')) \\\n",
    "        .filter(F.size('candidates') > 1) \\\n",
    "        .repartition(num_partitions) \\\n",
    "        .select('candidates') \\\n",
    "        .distinct() \\\n",
    "        .select(F.explode(combine_pairs('candidates')).alias('candidate_pair')) \\\n",
    "        .select(F.col('candidate_pair')[0].alias('candidate_pair_first'), F.col('candidate_pair')[1].alias('candidate_pair_second')) \\\n",
    "        .distinct() \n",
    "\n",
    "df_candidate_pairs = create_df_candidate_pairs(df_bands)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, before saving the results on disk, we remove the false positives, that is, the candidate pairs that have a Jaccard similarity, considering the shingles, above 85%.\n",
    "The results saved on disk will be useful to quickly query the `get_similar_articles` functions developed further for exercise 2.2.\n",
    "\n",
    "To verify if a given pair is a false positive, we calculate the Jaccard similarity documents' shingles in each pair using the `df_shingles` dataframe.\n",
    "This dataframe will be persisted in Spark using the default storage level (`MEMORY_AND_DISK`).\n",
    "It's not saved directly into disk since it has a size proportional to the source dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_candidate_pairs_fpless(df_candidate_pairs: DataFrame, df_shingles: DataFrame, similarity_threshold: float) -> DataFrame:\n",
    "    return df_candidate_pairs \\\n",
    "        .join(df_shingles, df_shingles['tweet_id'] == F.col('candidate_pair_first')) \\\n",
    "        .withColumnRenamed('shingles', 'shingles_first') \\\n",
    "        .drop('tweet_id') \\\n",
    "        .join(df_shingles, df_shingles['tweet_id'] == F.col('candidate_pair_second')) \\\n",
    "        .withColumnRenamed('shingles', 'shingles_second') \\\n",
    "        .drop('tweet_id') \\\n",
    "        .withColumn('similarity', F.size(F.array_intersect('shingles_first', 'shingles_second')) / F.size(F.array_union('shingles_first', 'shingles_second'))) \\\n",
    "        .drop('shingles_first', 'shingles_second') \\\n",
    "        .filter(F.col('similarity') >= similarity_threshold)\n",
    "\n",
    "df_shingles.cache()\n",
    "df_candidate_pairs_fpless = create_df_candidate_pairs_fpless(df_candidate_pairs, df_shingles, similarity_threshold)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This leaves us with the dataframe `df_candidate_pairs_fpless`, which will be composed of five columns: `candidate_pair_first`, `candidate_pair_second` and `similarity`.\n",
    "The results are saved into disk, considering the integer percentage of the similarity threshold that was initially defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname_candidate_pairs = f'candidate_pairs_{r}_{b}_{int(similarity_threshold * 100)}'\n",
    "if not os.path.exists(fname_candidate_pairs):\n",
    "    df_candidate_pairs_fpless.write.mode('overwrite').parquet(path=fname_candidate_pairs, compression='gzip')\n",
    "\n",
    "df_candidate_pairs_fpless = spark.read.parquet(fname_candidate_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procedure to get similar articles"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For exercise 2.2 we developed a function to get articles similar to a given article, identified by the tweet ID.\n",
    "\n",
    "In this function we filter all the pairs which have the given document/tweet_id and create a Python `list` with all the similar articles.\n",
    "\n",
    "Starting with the dataframe of candidate pairs without false positives, which is saved in disk, we transform both columns relating to the candidate pairs such that tweets equal to the queried-for tweet are turned to `null` (if `.otherwise()` is not specified after `.when()`, then all values that don't meet the condition in the `.when()` function are converted to `null`).\n",
    "\n",
    "Afterwards, all rows that don't have at least a `null` value in one of the columns (i.e. all candidate pairs that don't have the queried-for tweet) are filtered-out.\n",
    "Finally, both columns relating to the pair elements are coalesced, giving the first non-`null` value, which is the tweet that is not the queried-for tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_articles(tweet_id: str) -> List[str]:\n",
    "    rows = df_candidate_pairs_fpless \\\n",
    "        .withColumn('candidate_pair_first', F.when(F.col('candidate_pair_first') != tweet_id, F.col('candidate_pair_first'))) \\\n",
    "        .withColumn('candidate_pair_second', F.when(F.col('candidate_pair_second') != tweet_id, F.col('candidate_pair_second'))) \\\n",
    "        .filter(F.col('candidate_pair_first').isNull() | F.col('candidate_pair_second').isNull()) \\\n",
    "        .select(F.coalesce('candidate_pair_first', 'candidate_pair_second').alias('similar_article')) \\\n",
    "        .collect()\n",
    "\n",
    "    return [row.similar_article for row in rows]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of false positives/negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample of the dataset to use\n",
    "sample_fraction = 0.1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we load sample of the shingles data to do the analysis of false positives and negatives.\n",
    "\n",
    "Then we generate the candidate pairs like before.\n",
    "All necessary code was enclosed in functions, to facilitate this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shingles_sample = df_shingles.sample(fraction=sample_fraction, seed=seed, withReplacement=False)\n",
    "\n",
    "df_minhash_sample = create_df_minhash(df_shingles_sample)\n",
    "\n",
    "df_candidate_pairs_sample = create_df_candidate_pairs(create_df_bands(df_minhash_sample))\n",
    "\n",
    "df_candidate_pairs_fpless_sample = create_df_candidate_pairs_fpless(df_candidate_pairs_sample, df_minhash_sample, similarity_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### False positive percentage (false discovery rate)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have the dataframe of candidate pairs and the dataframe of candidate pairs without false positives, we can get the number of false positives by subtracting the number of rows of the dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of false positives: 19.322567%\n"
     ]
    }
   ],
   "source": [
    "candidate_pairs_n = df_candidate_pairs_sample.count()\n",
    "candidate_pairs_fpless_n = df_candidate_pairs_fpless_sample.count()\n",
    "print(f'Percentage of false positives: {(candidate_pairs_n - candidate_pairs_fpless_n) / candidate_pairs_n:%}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### False negative percentage (false omission rate)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get the false negative percentage, pairs between elements of the `df_shingles` dataframe are created (which are all possible pairs of documents) using a `crossJoin` with itself.\n",
    "\n",
    "Only sorted pairs are used, so that effectively equal pairs (such as `(A, B)` and `(B, A)`) and pairs of one element (such as `(A, A)`) are not used.\n",
    "They are sorted in ascending order, for proper comparison with the candidate pairs.\n",
    "\n",
    "Afterwards, we join with the dataframe of candidate pairs, with the goal of removing all rows that match with said dataframe (to get all pairs that weren't candidates).\n",
    "To do so, we perform a `left` join on the document pairs of both dataframes.\n",
    "\n",
    "Since it's a `left` join, rows that don't match with the `df_candidate_pairs_sample` dataframe will have `null` values for the columns `candidate_pair_first` and `candidate_pair_second`.\n",
    "Therefore, to get all rows that don't match with `df_candidate_pairs_sample`, we keep the rows with `null` values on the previously mentioned columns (it's enough to evaluate only one of them, since if one of them is `null` then the other is necessarily `null` as well).\n",
    "\n",
    "With this, the Jaccard similarity between the remaining pairs is calculated, using the shingles.\n",
    "Finally, the pairs are filtered in order to get those that surpassed the similarity threshold, and are thus false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "false_negatives = df_shingles_sample \\\n",
    "    .crossJoin(df_shingles_sample.select(F.col('tweet_id').alias('tweet_id_other'), F.col('shingles').alias('shingles_other'))) \\\n",
    "    .filter(F.col('tweet_id') < F.col('tweet_id_other')) \\\n",
    "    .join(df_candidate_pairs_sample, (F.col('tweet_id') == F.col('candidate_pair_first')) & (F.col('tweet_id_other') == F.col('candidate_pair_second')), 'left') \\\n",
    "    .filter(F.col('candidate_pair_first').isNull()) \\\n",
    "    .drop('candidate_pair_first', 'candidate_pair_second') \\\n",
    "    .withColumn('similarity', F.size(F.array_intersect('shingles_first', 'shingles_second')) / F.size(F.array_union('shingles_first', 'shingles_second'))) \\\n",
    "    .filter(F.col('similarity') >= similarity_threshold) \\\n",
    "    .count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the final percentage of false negatives we used the count previously calculated and divide it by the total number of negatives detected, which is obtained by subtracting the number of candidate pairs to the number of possible pairs of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 311:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of false negatives: 0.000000%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(f'Percentage of false negatives: {false_negatives / (math.comb(df_minhash_sample.count(), 2) - candidate_pairs_n):%}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
